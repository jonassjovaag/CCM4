#!/usr/bin/env python3
"""
Temporal Segmentation Module
=============================

Based on IRCAM research (Bujard et al. 2025):
"Learning Relationships between Separate Audio Tracks for Creative Applications"

Instead of frame-by-frame analysis, segments audio into musical gestures using:
- 250ms windows: Fine-grained for improvisation
- 500ms windows: Beat-aligned for structured music
- 350ms windows: Balanced compromise (recommended default)

This improves:
- Musical coherence (captures complete gestures)
- Relationship learning (better symbolic sequences)
- Pattern recognition (temporal context preserved)

Research findings:
- Smaller windows (250ms) â†’ better for free improvisation
- Larger windows (500ms) â†’ better for structured/metrical music
- 350ms â†’ good balance for both contexts
"""

import numpy as np
from typing import List, Tuple, Optional
from dataclasses import dataclass
from enum import Enum


class SegmentationMode(Enum):
    """Segmentation strategies based on musical context"""
    FINE_GRAINED = 250  # ms - For improvisation, detailed analysis
    BALANCED = 350      # ms - Good for both structured and free
    BEAT_ALIGNED = 500  # ms - For metrical/structured music


@dataclass
class AudioSegment:
    """A temporal segment of audio with metadata"""
    audio: np.ndarray       # The audio samples
    start_time: float       # Start time in seconds
    end_time: float         # End time in seconds
    segment_id: int         # Sequential ID
    duration_ms: float      # Actual duration in milliseconds
    sample_rate: int        # Sample rate


class TemporalSegmenter:
    """
    Segment audio into musical gestures instead of fixed frames
    
    Benefits over frame-by-frame:
    1. Captures complete musical events (not partial)
    2. Better temporal context for learning
    3. More efficient (fewer segments than frames)
    4. Aligned with human perception of musical time
    """
    
    def __init__(self, 
                 segment_duration_ms: float = 350.0,
                 overlap_ratio: float = 0.0,
                 min_segment_duration_ms: float = 100.0):
        """
        Initialize temporal segmenter
        
        Args:
            segment_duration_ms: Target segment duration in milliseconds
                                 250ms = fine-grained (improvisation)
                                 350ms = balanced (recommended)
                                 500ms = beat-aligned (structured music)
            overlap_ratio: Overlap between segments (0.0 = no overlap, 0.5 = 50% overlap)
            min_segment_duration_ms: Minimum segment duration (for edge cases)
        """
        self.segment_duration_ms = segment_duration_ms
        self.overlap_ratio = overlap_ratio
        self.min_segment_duration_ms = min_segment_duration_ms
        
        # Statistics
        self.total_segments_created = 0
        self.total_audio_processed = 0.0  # seconds
    
    def segment_audio(self, 
                     audio: np.ndarray, 
                     sr: int = 44100) -> List[AudioSegment]:
        """
        Segment audio into temporal windows
        
        Args:
            audio: Audio array
            sr: Sample rate
            
        Returns:
            List of AudioSegment objects
        """
        if len(audio) == 0:
            return []
        
        # Calculate segment size in samples
        segment_samples = int((self.segment_duration_ms / 1000.0) * sr)
        min_segment_samples = int((self.min_segment_duration_ms / 1000.0) * sr)
        
        # Calculate hop size (accounting for overlap)
        hop_samples = int(segment_samples * (1.0 - self.overlap_ratio))
        
        segments = []
        segment_id = 0
        position = 0
        
        while position < len(audio):
            # Extract segment
            end_position = min(position + segment_samples, len(audio))
            segment_audio = audio[position:end_position]
            
            # Only keep segments that meet minimum duration
            if len(segment_audio) >= min_segment_samples:
                start_time = position / sr
                end_time = end_position / sr
                duration_ms = (len(segment_audio) / sr) * 1000.0
                
                segment = AudioSegment(
                    audio=segment_audio,
                    start_time=start_time,
                    end_time=end_time,
                    segment_id=segment_id,
                    duration_ms=duration_ms,
                    sample_rate=sr
                )
                
                segments.append(segment)
                segment_id += 1
            
            # Move to next segment
            position += hop_samples
            
            # Prevent infinite loop if hop is too small
            if hop_samples == 0:
                position += 1
        
        # Update statistics
        self.total_segments_created += len(segments)
        self.total_audio_processed += len(audio) / sr
        
        return segments
    
    def segment_from_file(self, 
                         filepath: str, 
                         sr: int = 44100) -> Tuple[List[AudioSegment], dict]:
        """
        Load and segment audio file
        
        Args:
            filepath: Path to audio file
            sr: Target sample rate
            
        Returns:
            Tuple of (segments, file_info)
        """
        import librosa
        
        # Load audio
        audio, actual_sr = librosa.load(filepath, sr=sr)
        
        # Segment
        segments = self.segment_audio(audio, actual_sr)
        
        # File info
        file_info = {
            'filepath': filepath,
            'duration_seconds': len(audio) / actual_sr,
            'sample_rate': actual_sr,
            'total_segments': len(segments),
            'segment_duration_ms': self.segment_duration_ms,
            'overlap_ratio': self.overlap_ratio
        }
        
        return segments, file_info
    
    def get_statistics(self) -> dict:
        """Get segmentation statistics"""
        avg_segments_per_second = 0.0
        if self.total_audio_processed > 0:
            avg_segments_per_second = self.total_segments_created / self.total_audio_processed
        
        return {
            'total_segments_created': self.total_segments_created,
            'total_audio_processed_seconds': self.total_audio_processed,
            'average_segments_per_second': avg_segments_per_second,
            'segment_duration_ms': self.segment_duration_ms,
            'overlap_ratio': self.overlap_ratio
        }
    
    @staticmethod
    def get_recommended_mode(tempo: Optional[float] = None,
                           is_improvisation: bool = False,
                           is_structured: bool = False) -> SegmentationMode:
        """
        Get recommended segmentation mode based on musical context
        
        Args:
            tempo: Tempo in BPM (if known)
            is_improvisation: True for free improvisation
            is_structured: True for metrical/structured music
            
        Returns:
            Recommended SegmentationMode
        """
        # Free improvisation benefits from finer detail
        if is_improvisation:
            return SegmentationMode.FINE_GRAINED
        
        # Structured music benefits from beat alignment
        if is_structured:
            return SegmentationMode.BEAT_ALIGNED
        
        # If tempo is known, align to beat duration
        if tempo is not None:
            beat_duration_ms = (60.0 / tempo) * 1000.0
            
            # If beat is around 500ms, use beat alignment
            if 400 <= beat_duration_ms <= 600:
                return SegmentationMode.BEAT_ALIGNED
            # If beat is faster, use fine-grained
            elif beat_duration_ms < 400:
                return SegmentationMode.FINE_GRAINED
        
        # Default: balanced mode works well for most cases
        return SegmentationMode.BALANCED


def demo():
    """Demonstrate temporal segmentation"""
    import time
    
    print("=" * 70)
    print("Temporal Segmentation Module - Demo")
    print("=" * 70)
    print("\nBased on IRCAM research (Bujard et al. 2025)")
    print("Paper: 'Learning Relationships between Separate Audio Tracks'")
    
    # Generate 10 seconds of synthetic audio
    sr = 44100
    duration = 10.0
    t = np.linspace(0, duration, int(sr * duration))
    audio = np.sin(2 * np.pi * 440 * t)  # A4 tone
    
    print(f"\nðŸ“Š Test audio: {duration}s at {sr}Hz")
    
    # Test different segmentation modes
    modes = [
        ("Fine-grained (250ms)", 250),
        ("Balanced (350ms)", 350),
        ("Beat-aligned (500ms)", 500)
    ]
    
    for mode_name, duration_ms in modes:
        print(f"\n{'='*70}")
        print(f"ðŸŽµ {mode_name} Segmentation")
        print(f"{'='*70}")
        
        segmenter = TemporalSegmenter(segment_duration_ms=duration_ms)
        
        start_time = time.time()
        segments = segmenter.segment_audio(audio, sr)
        elapsed = time.time() - start_time
        
        print(f"   Segments created: {len(segments)}")
        print(f"   Segments/second: {len(segments) / duration:.1f}")
        print(f"   Processing time: {elapsed*1000:.2f}ms")
        
        # Show first 3 segments
        print(f"\n   First 3 segments:")
        for seg in segments[:3]:
            print(f"      Segment {seg.segment_id}: "
                  f"{seg.start_time:.3f}s - {seg.end_time:.3f}s "
                  f"({seg.duration_ms:.1f}ms, {len(seg.audio)} samples)")
    
    # Test recommendation system
    print(f"\n{'='*70}")
    print("ðŸŽ¯ Segmentation Mode Recommendations")
    print(f"{'='*70}")
    
    contexts = [
        ("Free Jazz Improvisation", None, True, False),
        ("Classical Symphony", 120, False, True),
        ("Unknown Context", None, False, False),
        ("Fast Dance Music", 140, False, True),
        ("Slow Ballad", 60, False, True)
    ]
    
    for context_name, tempo, is_improv, is_structured in contexts:
        mode = TemporalSegmenter.get_recommended_mode(tempo, is_improv, is_structured)
        tempo_str = f"{tempo} BPM" if tempo else "Unknown tempo"
        print(f"   {context_name:30s} ({tempo_str:12s}) â†’ {mode.name:15s} ({mode.value}ms)")
    
    print("\n" + "=" * 70)
    print("âœ… Temporal segmentation captures complete musical gestures!")
    print("=" * 70)


if __name__ == "__main__":
    demo()
