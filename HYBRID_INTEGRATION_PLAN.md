# Hybrid Integration Plan - Best of Both Worlds

## Combining Our Work + IRCAM Research (Bujard et al. 2025)

### What We're Building:

**Next-generation perception system** that combines:
1. **Our ratio-based analysis** (interpretable, psycho acoustic)
2. **Our harmonic-aware chroma** (CQT + temporal correlation)
3. **IRCAM's symbolic quantization** (efficient, learnable)
4. **IRCAM's temporal segmentation** (musical gesture capture)

---

## Architecture Overview

```
Audio Input
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HYBRID PERCEPTION MODULE (NEW!)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  [Harmonic-Aware Chroma]  (Our contribution)            â”‚
â”‚     â€¢ CQT-based (log frequency spacing)                 â”‚
â”‚     â€¢ Harmonic weighting (suppress overtones)           â”‚
â”‚     â€¢ Temporal correlation                              â”‚
â”‚     â†’ 12D chroma vector                                 â”‚
â”‚                                                           â”‚
â”‚  [Ratio Analyzer]  (Our contribution)                   â”‚
â”‚     â€¢ Frequency ratios (4:5:6 for major)               â”‚
â”‚     â€¢ Consonance scoring (Helmholtz 1877)              â”‚
â”‚     â€¢ Interval analysis                                 â”‚
â”‚     â†’ 10D ratio features                                â”‚
â”‚                                                           â”‚
â”‚  [Spectral Features]  (Standard)                        â”‚
â”‚     â€¢ RMS, centroid, rolloff, etc.                     â”‚
â”‚     â†’ 6D spectral features                              â”‚
â”‚                                                           â”‚
â”‚  CONCATENATE â†’ 28D feature vector                       â”‚
â”‚                                                           â”‚
â”‚  [Vector Quantization]  (IRCAM contribution)            â”‚
â”‚     â€¢ K-means (vocabulary = 64 classes)                 â”‚
â”‚     â€¢ Creates symbolic "musical alphabet"               â”‚
â”‚     â†’ Single token ID (0-63)                            â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Dual Output:
  1. Continuous features (28D) â†’ ML Classifier
  2. Symbolic token (1D) â†’ AudioOracle/Memory
```

---

## Implementation Status

### âœ… Completed:

1. **`listener/ratio_analyzer.py`**
   - Frequency ratio calculation
   - Consonance scoring
   - Chord type matching by ratios

2. **`listener/harmonic_chroma.py`**
   - CQT-based chroma
   - Harmonic weighting
   - Temporal correlation
   - Guided peak search

3. **`listener/symbolic_quantizer.py`**
   - K-means vector quantization
   - Vocabulary learning (16-256 classes)
   - Token encoding/decoding
   - Codebook statistics

4. **`listener/hybrid_perception.py`**
   - Integrates all three modules
   - Produces dual output (continuous + symbolic)
   - Ready for integration

### ğŸ”„ In Progress:

5. **Integration into Chandra_trainer**
6. **Integration into MusicHal_9000**
7. **Temporal segmentation (250-500ms windows)**
8. **TPP evaluation metrics**

---

## Integration Points

### 1. Chandra_trainer Enhancement

**Current flow**:
```python
Audio file â†’ Extract events â†’ Train AudioOracle
```

**Enhanced flow**:
```python
Audio file â†’ Extract events with hybrid perception
           â†’ Features: 28D (chroma + ratio + spectral)
           â†’ Symbolic tokens: For AudioOracle efficiency
           â†’ Train vocabulary (K-means codebook)
           â†’ Train AudioOracle on symbolic tokens
           â†’ Save: model.json + vocabulary.pkl
```

**Changes needed**:
- Add hybrid perception to event extraction
- Learn symbolic vocabulary from all events
- Store both continuous features + symbolic tokens
- Use tokens for more efficient AudioOracle

### 2. MusicHal_9000 Enhancement

**Current flow**:
```python
Live audio â†’ DriftListener (YIN + spectral)
           â†’ AudioOracle pattern matching
           â†’ AI Agent decision
           â†’ MIDI output
```

**Enhanced flow**:
```python
Live audio â†’ DriftListener (existing)
           â†’ Hybrid Perception (NEW!)
              â”œâ”€ Chroma features
              â”œâ”€ Ratio features (consonance!)
              â”œâ”€ Symbolic token
              â””â”€ Musical insights
           â†’ AudioOracle pattern matching (using tokens)
           â†’ AI Agent decision (with consonance awareness)
           â†’ MIDI output (harmonically informed)
```

**Benefits**:
- Richer feature representation
- More efficient memory (symbolic tokens)
- Consonance-aware decision making
- Interpretable musical parameters

### 3. Ratio-Based Chord Trainer

**Already implemented** - feeds into the hybrid perception!

The chord trainer validates that our ratio approach works (100% success!),
and these features now get integrated into the full system.

---

## Key Improvements from IRCAM Paper

### 1. Symbolic Vocabulary (Implemented âœ“)

**From paper**: Vocabulary size 16-64 optimal for learning

**Our implementation**:
- 64-class vocabulary (sweet spot)
- K-means clustering
- Entropy tracking
- Token frequency analysis

**Integration**:
```python
# In Chandra_trainer:
perception = HybridPerceptionModule(vocabulary_size=64)

# Extract features
for event in audio_events:
    result = perception.extract_features(event.audio)
    
    # Store both:
    continuous_features = result.features  # For ML
    symbolic_token = result.symbolic_token  # For AudioOracle
```

### 2. Temporal Segmentation

**From paper**: 250-500ms segments better than frame-by-frame

**Current**: Frame-by-frame (256 samples = 6ms)

**Enhancement**:
```python
class TemporalSegmenter:
    """Segment audio into musical gestures"""
    
    def segment(self, audio, segment_duration_ms=350):
        """
        Segment audio into uniform windows
        
        Args:
            segment_duration_ms: 250ms (improvisation) or 500ms (structured)
        """
        # Paper found 350ms works well for both!
```

### 3. Learned Relationships

**From paper**: Transformer learns Track A â†’ Track B mappings

**Could add to MusicHal_9000**:
```python
class RelationshipLearner:
    """Learn 'when performer plays X, respond with Y'"""
    
    def train_on_duos(self, performer_track, ai_track):
        """
        Learn from recordings of performer + AI
        Similar to paper's paired tracks approach
        """
```

---

## Immediate Next Steps (Implementing Now)

1. âœ… Create `symbolic_quantizer.py` (DONE)
2. âœ… Create `hybrid_perception.py` (DONE)
3. â³ Add to Chandra_trainer (IN PROGRESS)
4. â³ Add to MusicHal_9000 (NEXT)
5. â³ Add temporal segmentation (NEXT)

---

## Expected Improvements

### Chandra_trainer:
- **Before**: AudioOracle with 15D continuous features
- **After**: AudioOracle with 64-class symbolic tokens + 28D features for validation
- **Benefit**: More efficient memory, clearer patterns

### MusicHal_9000:
- **Before**: Pattern matching with hand-crafted features
- **After**: Pattern matching + ratio analysis + consonance-aware decisions
- **Benefit**: More musical responses, interpretable behavior

### Chord Analyzer:
- **Before**: Standalone validation tool
- **After**: Integrated into training pipeline, provides ratio features
- **Benefit**: Unified system with mathematical grounding

---

## Files Created/Modified

### New Files:
1. `listener/symbolic_quantizer.py` (285 lines) âœ…
2. `listener/hybrid_perception.py` (250 lines) âœ…
3. `HYBRID_INTEGRATION_PLAN.md` (this file)

### Files to Modify:
4. `Chandra_trainer.py` - Add hybrid perception
5. `MusicHal_9000.py` - Add hybrid perception
6. `memory/polyphonic_audio_oracle.py` - Add symbolic token support

---

## Testing Strategy

1. **Unit test** each module (symbolic_quantizer, hybrid_perception) âœ“
2. **Integration test** with Chandra_trainer on small audio file
3. **Validation** with ratio_based_chord_validator results (588 chords!)
4. **Live test** with MusicHal_9000 in performance
5. **Comparison**: Old vs new system accuracy/efficiency

---

## Timeline

- **Phase 1** (Today): Core modules + Chandra integration
- **Phase 2** (Tomorrow): MusicHal integration + testing
- **Phase 3** (This week): Temporal segmentation + optimization
- **Phase 4** (Next week): Evaluation + refinement

---

**Status**: ğŸŸ¢ Active implementation in progress!






























