# Real-Input Performance Analysis (01:27 Session)

## Executive Summary

**‚úÖ SYSTEM WORKING CORRECTLY** - Autonomous mode functional, good generation rate, responding to human input.

**Session**: 2.29 minutes (137 seconds)  
**Generation Rate**: 30.5 notes/minute (76% of target 40/min)  
**Human Input**: 19 events (8.3/min)  
**AI Output**: 70 events (3.68:1 ratio to human)  
**Response Time**: 0.05s min, 7.8s max, 3.6s average

---

## Key Observations from Console Output

### ‚úÖ POSITIVE FINDINGS

1. **Gesture Token Detection Working**
   ```
   üéµ Gesture token: 1
   üéπ D#5 | CHORD: min7 (73%) | Consonance: 0.72
   ```
   - System correctly detecting human input
   - MERT encoder quantizing to gesture tokens
   - Chord detection functioning (min7 at 73% confidence)

2. **Dual Vocabulary Mode Active**
   ```
   üéµ Dual vocab: Hybrid input ‚Üí contextual filling (harm=1, perc=1)
   ```
   - Harmonic and percussive vocabularies both engaged
   - Contextual filling strategy working

3. **RhythmOracle Integration Working**
   ```
   ü•Å RhythmOracle phrasing: pattern=pattern_1, duration=[2, 2, 2, 2], 
      tempo=60.0 BPM, density=2.19, similarity=0.88
   ```
   - Rhythm patterns learned and recalled
   - Tempo estimation functioning (60 BPM)
   - High similarity matching (0.88)

4. **Behavioral Modes Switching**
   ```
   üé≠ Mode shift: CONTRAST (will persist for 19s)
   üé≠ Mode shift: COUPLE (will persist for 19s)
   üé≠ Mode shift: SHADOW (will persist for 19s)
   üé≠ Mode shift: MIRROR (will persist for 25s)
   ```
   - Sticky modes working (19-26s persistence)
   - 4 different modes used in session
   - Mode distribution: SHADOW (37.1%), CONTRAST (32.9%), COUPLE (15.7%), MIRROR (12.9%)

5. **Oracle Generation Successful**
   ```
   üéº Generated 2 frames from Oracle
   üîç Oracle returned: 2 frames
   ‚úÖ Interval Translation: 1 intervals ‚Üí 2 MIDI notes
   ```
   - AudioOracle successfully generating notes
   - Interval extraction working
   - MIDI translation functional

6. **Voice Independence**
   - Melodic: 39 notes (55.7%)
   - Bass: 31 notes (44.3%)
   - Balanced voice distribution

### ‚ö†Ô∏è  AREAS OF CONCERN

1. **Low Gesture Diversity**
   ```
   ‚ö†Ô∏è LOW GESTURE DIVERSITY: Most tokens are similar ([1, 1, 1, 1, 1, 1])
   ‚ö†Ô∏è LOW GESTURE DIVERSITY: Most tokens are similar ([45, 45, 45, 45, 45, 45])
   ```
   - System stuck on token 1 for extended periods
   - Then stuck on token 45
   - Only 2 gesture tokens seen in entire session (1 and 45)
   - **Possible cause**: Input audio very consistent (sustained tone/drone?)
   - **Impact**: Limits musical variety

2. **Dual Vocab Not Matching Frames**
   ```
   üîç Dual vocab found 0 matches - will use parameter filtering on 1500 frames
   üîç Parameter filtering: consonance, target=0.65, tolerance=0.35
   ```
   - **EVERY query** falls back to parameter filtering
   - Dual vocabulary matching never succeeds (0 matches)
   - This means gesture tokens don't match training data
   - **Possible cause**: 
     - Training data used different MERT extraction settings?
     - Gesture token quantization mismatch?
     - Input significantly different from training material?

3. **Waiting Periods**
   ```
   ‚è≥ Waiting 21.0s before next generation (interval=22.1s, activity=0.79)
   ‚è≥ Waiting 21.5s before next generation (interval=23.0s, activity=0.83)
   ```
   - Long waits during high activity (0.79-0.85)
   - 20+ second gaps reduce generation rate
   - **Explains** why only 30.5 notes/min instead of 40+
   - **Cause**: Autonomous interval scaling based on activity level

4. **Bass Note Repetition**
   ```
   Most common bass notes:
      C2 (MIDI 36): 13x (41.9%)  ‚Üê 42% of all bass notes!
   ```
   - Significant bass repetition (42% same note)
   - Though diversity improved from 54.4% baseline to 41.9%
   - Still room for improvement

5. **Melodic Diversity Lower Than Expected**
   ```
   Unique notes: 14
   Diversity: 35.9%
   ```
   - Only 14 unique notes in 39 melodic events
   - 35.9% diversity means ~3 notes repeated frequently

---

## Detailed Analysis

### Generation Rate Breakdown

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Total notes/min | 30.5 | 40 | ‚ö†Ô∏è 76% of target |
| Melodic notes/min | 17.0 | ~20 | ‚ö†Ô∏è 85% of target |
| Bass notes/min | 13.5 | ~20 | ‚ö†Ô∏è 68% of target |
| Response latency | 3.6s avg | <5s | ‚úÖ Good |

**Why below target?**
- Long waiting periods (20+ seconds) due to high activity (0.79-0.85)
- Autonomous interval scaling: `base_interval * (1 + activity_level)`
- At 0.83 activity: `3s * (1 + 0.83) = 5.5s` between attempts
- Plus phrase pausing (2s after phrase complete)
- **Solution**: Tune activity scaling formula or reduce base interval

### Mode Distribution Analysis

| Mode | Count | % | Musical Intent |
|------|-------|---|----------------|
| SHADOW | 26 | 37.1% | Follow/complement human |
| CONTRAST | 23 | 32.9% | Diverge/contrast with human |
| COUPLE | 11 | 15.7% | Synchronized/harmonious |
| MIRROR | 9 | 12.9% | Direct imitation |
| IMITATE | 1 | 1.4% | (rare occurrence) |

**Good variety** - modes switching appropriately, sticky persistence working (19-26s).

### Gesture Token Issue

**Critical finding**: Only 2 gesture tokens appeared (1 and 45).

Trace through console:
```
Event 1:  üéµ Gesture token: 1
Event 2:  üéµ Gesture token: 1
Event 3:  üéµ Gesture token: 1
...
Event 9:  üéµ Gesture token: 45
Event 10: üéµ Gesture token: 45
...
```

**Hypothesis 1**: Input is sustained drone/tone
- Would explain consistent token 1
- Change to token 45 might be timbre/register shift

**Hypothesis 2**: MERT extraction mismatch
- Training used different settings than runtime
- Vocabulary quantization boundaries don't align

**Hypothesis 3**: Input far from training data
- User playing style very different from Itzama.wav
- MERT embeddings cluster differently

**Evidence from dual vocab failure**:
```
üîç Dual vocab found 0 matches - will use parameter filtering on 1500 frames
```
This repeats for EVERY query, meaning gesture tokens (1, 45) never match the 1500 trained frames.

---

## Performance vs Test Runs

### Automated Test (No Human Input)
- Duration: 2.02 minutes
- Generation rate: **153.2 notes/minute** 
- Allowed: 93.9%
- Blocking: 6.1% (mostly interval_too_soon)

### Real Input Session (01:27)
- Duration: 2.29 minutes
- Generation rate: **30.5 notes/minute** (80% LOWER!)
- Human input: 19 events
- AI/Human ratio: 3.68:1

**Why such a difference?**

1. **Activity-based waiting**: High human activity (0.79-0.85) triggers long waits
2. **Give-space factor**: System respecting human input
3. **Episode managers vs autonomous**: May be different code paths active
4. **SomaxBridge filtering**: "give-and-take" behavior causing pauses

---

## Recommendations

### Immediate Fixes

1. **Enable timing logger for real sessions**
   ```bash
   ENABLE_TIMING_LOGGER=1 python MusicHal_9000.py --enable-somax --enable-meld
   ```
   This will reveal exact blocking reasons during human interaction.

2. **Investigate dual vocabulary matching failure**
   - Check MERT extraction settings match training
   - Verify gesture quantization vocabulary files
   - Consider re-training with current MERT settings

3. **Tune activity-based interval scaling**
   Current: `autonomous_interval * (1 + activity_level)`
   At 0.83 activity: 3s * 1.83 = 5.5s between attempts
   
   Suggested: `autonomous_interval * (1 + activity_level * 0.5)`
   At 0.83 activity: 3s * 1.42 = 4.2s between attempts
   
   Or cap the scaling:
   ```python
   max_interval = min(autonomous_interval * (1 + activity_level), 6.0)
   ```

4. **Reduce bass repetition**
   - C2 (MIDI 36) appears 42% of the time
   - Add penalty for recently used notes
   - Increase diversity threshold in request masking

### Further Investigation

1. **Check what audio the user is playing**
   - Is it sustained tones? (would explain token 1 repetition)
   - Is it percussive? (dual vocab should see perc tokens)
   - Record a test and analyze with MERT manually

2. **Compare MERT vocabularies**
   ```bash
   # Check vocabulary files
   ls -lh input_audio/*_vocab.joblib
   # Inspect vocabulary parameters
   ```

3. **Run with debug logging**
   ```bash
   python MusicHal_9000.py --enable-somax --enable-meld --debug-decisions
   ```
   Will show real-time decision reasoning.

4. **Test with training audio**
   Play Itzama.wav back to the system and see if:
   - Gesture tokens match
   - Dual vocab finds matches
   - Generation rate improves

---

## Conclusion

**System Status**: ‚úÖ **OPERATIONAL**

The autonomous mode fix is working correctly. The system:
- ‚úÖ Responds to human input (3.6s avg latency)
- ‚úÖ Switches behavioral modes appropriately
- ‚úÖ Generates balanced voices (55% melodic, 45% bass)
- ‚úÖ Uses RhythmOracle patterns
- ‚úÖ Applies request masking

**Performance Issues**:
- ‚ö†Ô∏è Generation rate 76% of target (30.5 vs 40 notes/min)
- ‚ö†Ô∏è Dual vocabulary never matches (0/‚àû queries)
- ‚ö†Ô∏è Low gesture diversity (only 2 tokens seen)
- ‚ö†Ô∏è Long waiting periods during high activity

**Primary Bottleneck**: Activity-based interval scaling creates 20+ second gaps.

**Primary Mystery**: Why does dual vocabulary never match? This needs investigation.

**Next Steps**:
1. Run with `ENABLE_TIMING_LOGGER=1` to see exact blocking
2. Investigate dual vocab matching failure
3. Tune activity-based interval scaling
4. Test with training audio to verify matching

The fix we implemented (moving `set_autonomous_mode(True)` after `start()`) is working perfectly. The performance issues are about tuning generation parameters, not autonomous mode initialization.
