# Cross-Substrate Communication: Building Bridges Between Incommensurable Perceptual Systems

**Date:** 21 November 2025  
**Context:** Follow-up to "Perceptual Theory vs Human Theory" discussion  
**Status:** Implementation planning document  
**Branch:** compare-with-pre-refactor

---

## Table of Contents

1. [The Fundamental Challenge](#fundamental-challenge)
2. [Current System Architecture Analysis](#current-architecture)
3. [Current Communication Mechanisms](#current-communication)
4. [Communication Gaps and Failures](#communication-gaps)
5. [Proposed Communication Bridge Architecture](#proposed-architecture)
6. [Implementation Roadmap](#implementation-roadmap)
7. [Research Questions and Open Problems](#research-questions)
8. [Technical Specifications](#technical-specifications)

---

## 1. The Fundamental Challenge {#fundamental-challenge}

### Two Separate Beings with Incommensurable Substrates

**The core realization from yesterday's discussion:**

We have **two cognitive entities** attempting to make music together:

**Entity 1: Human Musician**
- Perceives: Chord names (Cmaj7, Dm7), functional harmony (I-IV-V-I), scales, keys
- Thinks: "That's a dominant seventh resolving to tonic"
- Communicates: Through learned symbolic music theory
- Embodied knowledge: Fingers, ears, years of musical training

**Entity 2: Machine Musical Intelligence**
- Perceives: 768D MERT embeddings â†’ Gesture tokens (0-63), Consonance scores (0.0-1.0), Frequency ratios
- Thinks: "Token 42 (consonance 0.73) â†’ Token 17 (consonance 0.68) with 87% transition probability"
- Communicates: Through learned perceptual patterns
- Embodied knowledge: AudioOracle graph, 500 training states, suffix links

**The problem:** These substrates are **not translatable** in the conventional sense.

- You cannot perfectly map Token 42 â†’ "Cmaj" (one-to-many: Token 42 appears with multiple chord types)
- You cannot perfectly map "Cmaj" â†’ Token 42 (many-to-one: Different Cmaj voicings map to different tokens)
- The 768D perceptual space is **richer** than symbolic chord labels (captures timbre, register, dynamics)
- Symbolic music theory is **more abstract** than perceptual features (functional relationships transcend specific sounds)

**But they MUST communicate to improvise together.**

### The Solution Cannot Be Translation

Yesterday's key insight: **Stop trying to translate. Build communication protocols that work ACROSS substrates.**

Like two people speaking different languages who develop shared gestures, context-dependent signals, and mutual understanding through accumulated interactionâ€”without one learning to "speak" the other's language.

**This requires:**
1. **Bidirectional transparency** - Machine shows its perceptual state, human shows their intent
2. **Embodied learning** - Human develops fluency with token space through exposure
3. **Shared gesture vocabulary** - Common reference points discovered through interaction
4. **Intentionality signaling** - Musical parameters (dynamics, timing, timbre) carry meta-communicative meaning
5. **Uncertainty acknowledgment** - Both entities indicate when they "don't understand"

---

## 2. Current System Architecture Analysis {#current-architecture}

### 2.1 Training Pipeline Output (General_idea.json - 50MB)

**Most recent training:** `General_idea.wav` (95MB audio, 339 seconds)

**Training results:**
- **AudioOracle:** 501 states, 247,264 transitions, 500 suffix links
- **Audio frames:** 500 frames with 768D MERT embeddings + audio data
- **Consonances:** 500 consonance scores (0.0-1.0)
- **RhythmOracle:** 3 rhythmic patterns (dense, medium, sparse)
- **Events processed:** 10,923 (from 49,865 total, hierarchically sampled)
- **Distance function:** Cosine similarity, threshold 0.35
- **Feature dimensions:** 768 (MERT embeddings)

**File structure:**
```json
{
  "metadata": {
    "version": "2.0",
    "created_at": "2025-11-20T22:24:47",
    "training_source": "input_audio/General_idea.wav",
    "git_commit": "54dead1f",
    "git_branch": "compare-with-pre-refactor"
  },
  "data": {
    "training_successful": true,
    "events_processed": 10923,
    "harmonic_patterns": 49865,
    "audio_oracle": {
      "format_version": "2.0",
      "distance_threshold": 0.35,
      "distance_function": "cosine",
      "feature_dimensions": 768,
      "states": {...},           // 501 states (Factor Oracle graph nodes)
      "transitions": {...},      // 247,264 edges
      "suffix_links": {...},     // 500 links (pattern repetitions)
      "audio_frames": {...},     // 500 frames with features + audio
      "consonances": {...},      // 500 consonance scores
      "fundamentals": {...}      // Root frequencies (currently empty)
    },
    "rhythm_oracle": {
      "patterns": [...],         // 3 learned rhythmic patterns
      "transitions": {...},
      "frequency": {...},
      "timestamp": 1763677483.26
    }
  }
}
```

**Critical observations:**

1. **No chord labels in core data** - AudioOracle works purely with:
   - 768D MERT embeddings (in `audio_frames[i].features`)
   - Consonance scores (in `consonances[i]`)
   - State transitions (probabilistic graph)

2. **Feature data is continuous** - 768D vectors stored as JSON lists:
   ```json
   "features": [-0.046, -0.218, 0.131, -0.079, -0.308, ...]
   ```

3. **State structure is minimal** - Each state is just graph topology:
   ```json
   "states": {
     "0": {
       "len": 0,
       "link": -1,
       "next": {"0": ..., "1": ..., ...}
     }
   }
   ```

4. **Audio frames separate from states** - Features stored in `audio_frames`, not in `states` directly. This is efficient serialization.

### 2.2 Vocabulary Files (Joblib - 1.1MB each)

**Files:** `General_idea_harmonic_vocab.joblib`, `General_idea_percussive_vocab.joblib`

**Structure:**
```python
{
  'kmeans': MiniBatchKMeans(n_clusters=64, ...),
  'scaler': None,
  'codebook': ndarray(64, 128),        # 64 tokens Ã— 128D PCA-reduced space
  'vocabulary_size': 64,
  'feature_dim': 768,                  # Original MERT dimension
  'token_frequencies': {0: 423, 1: 567, ...},  # Usage statistics
  'use_l2_norm': True,
  'pca': PCA(n_components=128),
  'use_pca': True,
  'pca_components': 128,
  'reduced_dim': 128,
  'min_cluster_separation': 0.1
}
```

**Key insights:**

1. **Dual vocabulary system** - Separate codebooks for harmonic vs percussive content (HPSS separation)

2. **PCA dimensionality reduction** - 768D MERT â†’ 128D PCA â†’ 64 KMeans clusters
   - Preserves 768D for training but quantizes via 128D intermediate space
   - Speeds up distance calculations in live performance

3. **Token frequencies tracked** - System knows which tokens appear often (common gestures) vs rarely (unique moments)

4. **L2 normalization** - Features normalized before clustering (focuses on direction, not magnitude)

5. **Cluster separation enforced** - Minimum 0.1 distance between cluster centers (prevents degenerate clusters)

### 2.3 Machine Music Theory Components

**Vocabulary (64 gesture tokens per modality)**
- Token 0-63 (harmonic) - Learned perceptual categories for tonal content
- Token 0-63 (percussive) - Learned perceptual categories for rhythmic attacks
- Each token = cluster center in 128D PCA-reduced space
- Tokens are **discrete symbols** but represent **continuous perceptual similarity**

**Harmony (consonance scores)**
- Mathematical frequency ratio analysis (FrequencyRatioAnalyzer)
- Continuous values 0.0 (dissonant) to 1.0 (consonant)
- Based on psychoacoustic principles (simple ratios = consonant)
- Stored per audio frame in `consonances` dict

**Syntax (AudioOracle transitions)**
- 247,264 learned transitions between 501 states
- Probabilistic graph: State A â†’ State B with learned probability
- Suffix links enable pattern jumping (similar past contexts)
- Distance threshold 0.35 determines "similar enough" for suffix link

**Memory (500 audio frames)**
- Each frame: timestamp, 768D embedding, audio segment, consonance
- Frames indexed 0-499 (circular buffer during training)
- AudioOracle states reference these frames
- Enables pattern provenance ("this came from frame 234 at timestamp 1763676200")

**Semantics (learned from training corpus)**
- Token meanings emerge from:
  - Cluster membership (which audio maps to this token?)
  - Transition patterns (which tokens follow this token?)
  - Consonance correlation (does this token predict high/low consonance?)
  - Temporal context (when in the piece does this token appear?)

### 2.4 Human Music Theory Overlay (Current Translation Layer)

**Ensemble chord voting** (`MusicHal_9000.py` lines 1633-1711):

```python
candidates = []

# Source 1: Wav2Vec chord classifier (ML model, weight 1.0x)
if wav2vec_chord_classifier:
    chord, conf = classifier.predict(wav2vec_features)
    candidates.append((chord, conf, conf * 1.0, 'W2V'))

# Source 2: Ratio-based (frequency ratios â†’ chord template, weight 1.2x)
if ratio_chord:
    candidates.append((ratio_chord, ratio_conf, ratio_conf * 1.2, 'ratio'))

# Source 3: ML hybrid model (RandomForest, weight 1.0x)
if ml_chord:
    candidates.append((ml_chord, ml_conf, ml_conf, 'ML'))

# Source 4: Harmonic context (chroma â†’ template match, weight 0.8x)
if harmonic_context.current_chord:
    candidates.append((hc_chord, hc_conf, hc_conf * 0.8, 'HC'))

# Pick winner by weighted confidence
best = max(candidates, key=lambda x: x[2])
display_chord = best[0]  # e.g., "Cmaj"
```

**Critical observation:** This is **ONLY for display**. Generation never uses `display_chord`.

**What generation actually uses:**
```python
request = {
    'gesture_token': current_token,      # e.g., 42 (from quantizer)
    'consonance': target_consonance,     # e.g., 0.7 (from ratio analysis)
    'rhythm_ratio': (3, 2),              # Brandtsegg temporal pattern
}

next_state = audio_oracle.generate_with_request(request)
```

**The disconnect:** Human sees "Cmaj (75% confidence)", machine thinks "Token 42, consonance 0.73".

---

## 3. Current Communication Mechanisms {#current-communication}

### 3.1 Machine â†’ Human Communication

**Real-time displays (PyQt5 GUI Dashboard):**

**Status Bar Viewport:**
- Behavior mode: SHADOW/MIRROR/COUPLE with color coding
- Mode duration: Countdown timer (e.g., "0:45 remaining")
- Detected chord: "Cmaj (75%)" â† **TRANSLATION LAYER**
- Performance phase: "Building"/"Sustaining"/"Resolving"
- Elapsed time: "0:23"
- Update rate: 100ms

**Audio Analysis Viewport:**
- Waveform visualization (color-coded for onsets)
- Onset detection indicator
- **Detected chord** (prominent display) â† **TRANSLATION LAYER**
- Rhythm ratio: "[3, 2]" (Brandtsegg)
- Consonance: "0.73" â† **MACHINE NATIVE**
- Barlow complexity: "2.7"
- **Gesture token: "42"** â† **MACHINE NATIVE**
- Update rate: 30ms (smooth waveform)

**Request Parameters Viewport:**
- Current mode: Large badge (SHADOW/MIRROR/COUPLE)
- Duration countdown: Real-time timer
- Request structure: Primary/Secondary/Tertiary parameters with weights
- Temperature setting
- Update rate: 100ms

**Pattern Match Viewport:**
- Pattern match score: "87%"
- AudioOracle state ID: "State 234"
- Current gesture token: "Token 42"
- Recent context: Last 5 tokens, consonance values

**Terminal Status (one-line live update):**
```
ðŸŽ¹ E3 | CHORD: Cmaj (75%) | Consonance: 0.73 | MIDI: 143 notes | Events: 2450
```

**Decision transparency logs (CSV):**
```csv
timestamp, mode, voice_type, trigger_midi, trigger_note, context_tokens, context_consonance, request_primary, pattern_match_score, generated_notes, reasoning, confidence
1763669054.50, imitate, bass, 52, E3, [35], 0.00, :::, , 0.8, [60], "imitate mode response; Triggered by your E3...", 0.80
```

**Logged every decision** (comprehensive but post-hoc, not real-time visible).

### 3.2 Human â†’ Machine Communication

**Audio input pipeline** (`jhs_listener_core.py`):

```
Microphone/Audio Interface
    â†“
sounddevice.InputStream (512 samples @ 44.1kHz, ~11.6ms latency)
    â†“
Ring buffer (circular storage)
    â†“
YIN pitch detection (fundamental frequency F0)
    â†“
Onset detection (spectral flux)
    â†“
Feature extraction (centroid, rolloff, ZCR, HNR, MFCCs)
    â†“
Event object creation
    â†“
Hybrid perception extraction (30-50ms)
    â†“
MERT encoding (768D)
    â†“
PCA reduction (128D)
    â†“
KMeans quantization â†’ Gesture token (0-63)
    â†“
Gesture smoothing (1.5s temporal window)
    â†“
FrequencyRatioAnalyzer â†’ Consonance score
    â†“
Event data structure (machine representation)
```

**Event data passed to AI agent:**
```python
event_data = {
    'hybrid_wav2vec_features': [768D array],
    'hybrid_gesture_token': 42,           # â† MACHINE NATIVE
    'hybrid_consonance': 0.73,            # â† MACHINE NATIVE
    'hybrid_ratio_chord': "Cmaj",         # â† TRANSLATION (for display)
    'hybrid_ratio_confidence': 0.75,
    'harmonic_token': 42,                 # Dual vocabulary support
    'content_type': "harmonic"            # vs "percussive"/"hybrid"
}
```

**Manual override (Harmonic Context Manager):**
- User can force specific chord context
- Displayed as: `OVERRIDE: Fmaj (45s)`
- Flashing warning in viewport
- System detects actual chord but generation uses override

### 3.3 What's Transparent, What's Opaque

**Transparent (human can see):**
âœ… Behavioral mode (SHADOW/MIRROR/COUPLE)  
âœ… Mode duration remaining  
âœ… Detected chord name (ensemble voting result)  
âœ… Chord confidence percentage  
âœ… Consonance score (0.0-1.0)  
âœ… Gesture token number (e.g., "42")  
âœ… Rhythm ratio pattern (e.g., "[3, 2]")  
âœ… Pattern match score (e.g., "87%")  
âœ… Override warnings  

**Opaque (human cannot see during performance):**
âŒ **What Token 42 means** - No semantic explanation  
âŒ **Why this token was chosen** - Decision reasoning hidden  
âŒ **Pattern provenance** - Which training audio frame matched?  
âŒ **Ensemble disagreement** - When 4 chord sources conflict  
âŒ **Request filtering** - How many states were rejected?  
âŒ **Suffix link traversal** - Which path through AudioOracle graph?  
âŒ **Phrase memory activation** - Which motif was recalled?  
âŒ **Confidence breakdown** - Per-source chord detection scores  
âŒ **Feature space distance** - How similar is current state to matched state?  

---

## 4. Communication Gaps and Failures {#communication-gaps}

### 4.1 Machine â†’ Human Failures

**Problem 1: Token Opacity**

**Symptom:** Viewport shows "Token 42" but user has no idea what this means.

**Example:**
```
Audio Analysis: Token 42 | Consonance 0.73 | Chord: Cmaj (45%)
```

**Human thinks:** "Is Token 42 always Cmaj? What if it shows Cmaj7 next time? Is the token wrong or the chord label?"

**Machine reality:** Token 42 represents a cluster of perceptually similar 768D embeddings. It might appear with:
- Cmaj (open voicing, mid-register)
- Cmaj7 (with added 7th but similar timbre)
- Am (relative minor, similar consonance)
- Any audio with similar spectral characteristics

**The problem:** Token has **perceptual meaning** (sounds like X) not **symbolic meaning** (is chord Y).

**Impact:** User cannot develop intuition for token vocabulary without semantic grounding.

---

**Problem 2: Ensemble Confusion**

**Symptom:** Chord label displayed with high confidence, but sources disagree.

**Example:**
```
Terminal: CHORD: Cmaj (75%)
```

**Hidden reality:**
```
W2V:   Cmaj  (45% conf, weighted 45%)
Ratio: Cmaj7 (68% conf, weighted 82%)  â† Winner due to 1.2x boost
ML:    Am    (52% conf, weighted 52%)
HC:    C     (35% conf, weighted 28%)
```

**Human thinks:** "System is 75% sure it's Cmaj"

**Machine reality:** System is **uncertain**, sources disagree, winner only wins because of weighting scheme.

**The problem:** False precision hides epistemic uncertainty.

**Impact:** User trusts chord label when they shouldn't, gets confused when generation doesn't match label.

---

**Problem 3: Pattern Provenance Hidden**

**Symptom:** System generates interesting phrase, user has no idea where it came from.

**Example:**
```
Pattern Match: 87% | State 234
```

**Human thinks:** "87% match to... what? What's State 234?"

**Machine reality:** State 234 references audio frame 234:
- Timestamp: 1763676200.45 (absolute Unix time)
- Training position: Bar 47 of General_idea.wav (approximately)
- Audio context: [2-second audio buffer available]
- Features: [768D embedding available]

**The problem:** No way to **play back** the matched training audio, **visualize** the feature similarity, or **understand** what made this match score 87%.

**Impact:** User cannot learn from the machine's choices, cannot understand "why did it play that?"

---

**Problem 4: Request Filtering Invisible**

**Symptom:** User sees request parameters but not how they filtered states.

**Example:**
```
Request Parameters:
  gesture_token=42 (weight: 0.95)
  consonance>0.7 (weight: 0.5)
  rhythm_ratio=(3,2) (weight: 0.3)
```

**Hidden process:**
```
AudioOracle has 501 states total
  â†’ Filter by gesture_token=42: 127 candidates remain
  â†’ Filter by consonance>0.7: 23 candidates remain
  â†’ Filter by rhythm_ratio=(3,2): 8 candidates remain
  â†’ Sort by transition probability: [234, 456, 123, 89, ...]
  â†’ Select state 234 (highest probability: 0.62)
```

**Human thinks:** "System applied these parameters somehow"

**Machine reality:** Specific filtering pipeline with quantifiable results at each stage.

**The problem:** No visibility into **how constraints narrowed the search space**.

**Impact:** User cannot understand why some requests work (find matches) and others don't (no matches).

---

**Problem 5: Reasoning Not Real-Time**

**Symptom:** Decision explainer exists but not displayed during performance.

**Current state:**
- `DecisionExplanation` objects created
- Logged to CSV files
- Stored in memory buffer (last 100 decisions)
- **Never shown in GUI viewports**

**Example explanation (currently hidden):**
```
SHADOW mode: Close imitation
Triggered by your F4 (MIDI 65)
Context: gesture_token=142, consonance=0.73, melody: ascending
Request: match gesture_token==142, consonance > 0.7
Pattern match: 87% similarity to training bar 23
Generated: F4 â†’ G4 â†’ A4 â†’ A#4 (ascending, narrow range)
Confidence: 0.87
```

**Human experience:** Sees "Pattern match: 87%" in viewport, doesn't see full reasoning.

**The problem:** Rich explanation infrastructure exists but **not integrated into live display**.

**Impact:** User misses real-time transparency, must analyze CSV logs post-performance to understand decisions.

---

### 4.2 Human â†’ Machine Failures

**Problem 6: Intentionality Blindness**

**Symptom:** Machine cannot distinguish intentional musical gestures from artifacts.

**Examples:**

**Intentional pause vs thinking time:**
```
Human: [Plays phrase, pauses 3 seconds, continues]
Machine: Detects 3s silence â†’ Logs low activity â†’ No semantic understanding
```
Is the pause:
- Intentional phrasing (musical rest, creates tension)?
- Thinking time (human deciding what to play next)?
- Technical issue (cable unplugged, turned away from mic)?

Machine treats all silences identically.

**Question phrasing vs statement:**
```
Human: [Plays ascending melodic line with slight ritardando]
Machine: Detects rising pitch, IOI increasing, no semantic tag
```
Human intent: "This is a musical question, respond with an answer"
Machine interpretation: "Ascending gesture detected, match similar ascending patterns"

**Loud = emphasis vs loud = natural:**
```
Human: [Plays forte chord]
Machine: RMS = -15 dB, no intentionality tag
```
Is this:
- Emphatic statement ("pay attention to THIS")?
- Natural playing volume?
- Accidental hammer-on?

Machine has RMS values but no **semantic interpretation** of dynamics.

**The problem:** Musical parameters (pause, dynamics, timing, articulation) carry **meta-communicative meaning** in human music-making, but machine sees them as **raw audio features** without intentionality layer.

**Impact:** Machine misses "what the human is trying to say" beyond the notes.

---

**Problem 7: Harmonic Function Blindness**

**Symptom:** Machine detects chord names but not functional relationships.

**Example:**
```
Human plays: Dm7 â†’ G7 â†’ Cmaj (ii-V-I in C major)
Machine detects:
  Event 1: Token 23, consonance 0.68, chord "Dm7"
  Event 2: Token 17, consonance 0.42, chord "G7"
  Event 3: Token 42, consonance 0.85, chord "Cmaj"
```

**Human intent:** Classic jazz cadence, functional harmonic progression, teleological motion toward tonic resolution.

**Machine interpretation:** Three separate tokens with consonance trajectory 0.68 â†’ 0.42 â†’ 0.85 (dip and recovery), transition probabilities based on training corpus.

**The problem:** Machine learns **statistical patterns** (these tokens often follow in this order) but not **functional meaning** (subdominant â†’ dominant â†’ tonic resolution).

**Impact:** Machine can reproduce ii-V-I if it appears in training, but doesn't understand **why** humans play it, can't generalize to other keys or substitute chords functionally.

---

**Problem 8: Timbre as Meaning**

**Symptom:** Machine extracts timbral features (spectral centroid, rolloff, MFCCs) but doesn't interpret timbre as semantic signal.

**Examples:**

**Distorted guitar = aggression:**
```
Human: [Plays power chord with heavy distortion]
Machine: Spectral centroid = 4500 Hz (bright), rolloff = 6000 Hz
No semantic tag: "aggressive tone"
```

**Clean guitar = delicate:**
```
Human: [Plays arpeggiated chord fingerstyle]
Machine: Spectral centroid = 1800 Hz (warm), rolloff = 3000 Hz
No semantic tag: "delicate tone"
```

**The problem:** Timbre has **expressive intention** in performance (choosing distortion vs clean is a musical decision), but machine treats it as **acoustic property** without meaning.

**Impact:** Machine cannot respond appropriately to timbral gestures (match aggressive with aggressive, contrast delicate with forceful).

---

**Problem 9: Phrase Boundary Ambiguity**

**Symptom:** Machine uses onset density thresholds for phrase detection, misses nuanced boundaries.

**Example:**
```
Human: [Plays 4-bar phrase with slight breath pause at end]
Machine: Onset density drops, threshold triggered, "phrase boundary detected"
But:
- Is this a natural phrase boundary (musical syntax)?
- Or just a brief hesitation (human uncertainty)?
- Or preparation for next section (mental preparation)?

**Current detection:** Density threshold (e.g., <2 onsets/second for 1 second = boundary)

**The problem:** Musical phrasing is **contextual and intentional**, not just statistical density patterns.

**Impact:** Machine may detect boundaries where humans don't intend them (false positives) or miss subtle phrasing cues (false negatives).

---

## 5. Proposed Communication Bridge Architecture {#proposed-architecture}

### 5.1 Design Principles

**Principle 1: No Forced Translation**

Don't try to make machine "speak human" or human "speak machine". Build **bidirectional interfaces** that preserve each substrate's native representation.

**Bad:**
```
Machine token 42 â†’ Force label as "Cmaj" â†’ Display only "Cmaj"
```

**Good:**
```
Machine token 42 â†’ Display "Token 42" (primary)
                 â†’ Display "â‰ˆ Cmaj" (interpretive, uncertain)
```

**Principle 2: Graduated Transparency**

Show machine-native representation first, human interpretation second, with clear **epistemic status** markers.

**Hierarchy:**
1. **Ground truth** (machine perception): Token 42, consonance 0.73
2. **Interpretation** (best guess): "â‰ˆ Cmaj"
3. **Uncertainty** (confidence): "45% Cmaj vs 38% Cmaj7"
4. **Provenance** (source): "Ensemble voting (ratio-based winner)"

**Principle 3: Embodied Learning Over Translation**

Help human **develop fluency** with machine's perceptual vocabulary rather than constantly translating for them.

**Mechanisms:**
- Token exemplar audio playback ("click Token 42 â†’ hear representative sounds")
- Accumulated exposure ("you've heard Token 42 fifty times, starting to recognize it")
- Semantic annotation ("you label Token 42 as 'bluesy opening gesture'")
- Pattern visualization ("Token 42 usually goes to Token 17 or 23")

**Principle 4: Intentionality Through Musical Parameters**

Map existing musical dimensions (dynamics, timing, timbre, articulation) to **meta-communicative signals** rather than adding explicit "intent buttons".

**Examples:**
- **Loud dynamics** â†’ "This is important, match it closely"
- **Soft dynamics** â†’ "This is tentative, feel free to diverge"
- **Rubato timing** â†’ "Phrase boundary approaching"
- **Strict timing** â†’ "Locked groove, stay in rhythm"
- **Distorted timbre** â†’ "Aggressive energy"
- **Clean timbre** â†’ "Delicate, subtle"

**Principle 5: Uncertainty as Feature, Not Bug**

When machine is uncertain, **show the uncertainty**. When human is ambiguous, **acknowledge the ambiguity**.

**Visual indicators:**
- Flashing viewport border (pattern match <60%)
- Transparent/dimmed MIDI output (confidence <40%)
- Multiple candidate display (ensemble disagreement)
- Question mark badges ("system unsure")

### 5.2 Dual-Layer Transparency Viewports

**Redesign core viewports to separate machine truth from human interpretation.**

**Audio Analysis Viewport (revised):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MACHINE PERCEPTION (Primary)                  [ðŸ” Explain] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gesture Token: 42                          [â–¶ Play Audio]  â”‚
â”‚ Consonance:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 0.73 (73%)                  â”‚
â”‚ Rhythm Ratio:  [3, 2]                                       â”‚
â”‚ Content Type:  Harmonic                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ HUMAN INTERPRETATION (Best Guess)              [âš™ Sources] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chord: Cmaj                                                 â”‚
â”‚   âš ï¸  Low confidence - sources disagree:                    â”‚
â”‚   â€¢ Ratio:  Cmaj7  (68% conf) â† Winner (1.2x weight)       â”‚
â”‚   â€¢ W2V:    Cmaj   (45% conf)                               â”‚
â”‚   â€¢ ML:     Am     (52% conf)                               â”‚
â”‚   â€¢ HC:     C      (35% conf)                               â”‚
â”‚                                                              â”‚
â”‚ Key: C major (estimated)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Interactive elements:**

- **[ðŸ” Explain]** button â†’ Opens decision reasoning overlay
- **[â–¶ Play Audio]** button â†’ Plays exemplar audio for Token 42 from training corpus
- **[âš™ Sources]** button â†’ Expands ensemble breakdown (shown above)
- Click on **Consonance bar** â†’ Shows frequency ratio details
- Click on **Token 42** â†’ Opens token explorer (see below)

**Pattern Match Viewport (revised):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PATTERN MATCHING                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Match Score:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 87%                    â”‚
â”‚ Matched State:   #234 (from training)                       â”‚
â”‚ Provenance:      Bar ~47, timestamp 1763676200.45           â”‚
â”‚                  [â–¶ Play Training Audio]                    â”‚
â”‚                                                              â”‚
â”‚ Trajectory:      State 145 â†’ 234 â†’ 189 (suffix link)       â”‚
â”‚   [View Graph]                                              â”‚
â”‚                                                              â”‚
â”‚ Request Filtering:                                          â”‚
â”‚   Total states:        501                                  â”‚
â”‚   gesture_token=42:    127 candidates (filtered -374)       â”‚
â”‚   consonance>0.7:      23 candidates  (filtered -104)       â”‚
â”‚   rhythm_ratio=(3,2):  8 candidates   (filtered -15)        â”‚
â”‚   Best match:          State 234 (probability 0.62)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key transparency additions:**

1. **Provenance**: Shows which training audio matched ("Bar ~47")
2. **Playback**: Can hear the matched training segment
3. **Graph visualization**: Trace AudioOracle trajectory
4. **Filtering cascade**: See how request narrowed search space

**Request Parameters Viewport (revised):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI DECISION PARAMETERS                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mode: SHADOW                    Remaining: 0:45             â”‚
â”‚   "Close imitation of your input"                           â”‚
â”‚                                                              â”‚
â”‚ Request Structure:                                          â”‚
â”‚   PRIMARY (required):                                       â”‚
â”‚     â€¢ gesture_token = 42           (weight: 0.95)           â”‚
â”‚     â€¢ content_type = harmonic                               â”‚
â”‚                                                              â”‚
â”‚   SECONDARY (preferred):                                    â”‚
â”‚     â€¢ consonance = 0.70 Â± 0.10     (weight: 0.50)           â”‚
â”‚     â€¢ rhythm_ratio = (3, 2)        (weight: 0.30)           â”‚
â”‚                                                              â”‚
â”‚   TERTIARY (optional):                                      â”‚
â”‚     â€¢ register = mid               (weight: 0.10)           â”‚
â”‚     â€¢ density = medium             (weight: 0.05)           â”‚
â”‚                                                              â”‚
â”‚ Temperature: 0.7 (some variation)                           â”‚
â”‚                                                              â”‚
â”‚ Last Decision:                                              â”‚
â”‚   Match quality: 87%  [ðŸ“ Full Reasoning]                   â”‚
â”‚   Confidence:    0.82                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Improvements:**

1. **Hierarchical display**: Primary/Secondary/Tertiary clearly distinguished
2. **Human-readable mode explanation**: "Close imitation..."
3. **Last decision summary**: Quick access to reasoning
4. **Tolerance indicators**: Shows `Â± 0.10` for consonance range

### 5.3 Token Semantics Explorer (New Tool)

**Purpose:** Help human develop fluency with machine's 64-token perceptual vocabulary.

**Implementation:** New viewport/window (`visualization/token_explorer.py`)

**Interface:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TOKEN SPACE EXPLORER                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  [2D Token Map - t-SNE Projection]                          â”‚
â”‚                                                              â”‚
â”‚      42â—                      17â—                            â”‚
â”‚         â•²                    â•±                               â”‚
â”‚          â•²                  â•±                                â”‚
â”‚           â•²                â•±                                 â”‚
â”‚       23â—â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â—35                              â”‚
â”‚                  8â—                                          â”‚
â”‚                                                              â”‚
â”‚  Click any token to see details â†’                           â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOKEN 42 DETAILS                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cluster Info:                                               â”‚
â”‚   Frequency:       567 occurrences in training              â”‚
â”‚   Common contexts: After Token 8, 23, 35                    â”‚
â”‚                    Before Token 17, 19, 42                   â”‚
â”‚   Avg consonance:  0.74 (consonant tendency)                â”‚
â”‚                                                              â”‚
â”‚ Audio Exemplars:                                            â”‚
â”‚   [â–¶] Frame 12  (timestamp 1763676201.2)                    â”‚
â”‚   [â–¶] Frame 67  (timestamp 1763676245.8)                    â”‚
â”‚   [â–¶] Frame 234 (timestamp 1763676789.3)                    â”‚
â”‚   [â–¶] Frame 402 (timestamp 1763677001.1)                    â”‚
â”‚                                                              â”‚
â”‚ Transition Probabilities:                                   â”‚
â”‚   â†’ Token 17:  0.32 (most common)                           â”‚
â”‚   â†’ Token 23:  0.18                                          â”‚
â”‚   â†’ Token 42:  0.15 (self-loop)                             â”‚
â”‚   â†’ Token 8:   0.12                                          â”‚
â”‚   â†’ Other:     0.23                                          â”‚
â”‚                                                              â”‚
â”‚ Your Annotations:                                           â”‚
â”‚   "Bluesy opening gesture - warm, mid-register"             â”‚
â”‚   [Edit] [Add New]                                          â”‚
â”‚                                                              â”‚
â”‚ System Analysis:                                            â”‚
â”‚   Spectral: Bright (centroid 2500 Hz avg)                   â”‚
â”‚   Temporal: Medium attack, sustained                        â”‚
â”‚   Harmonic: Major-ish quality (ratio patterns)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Features:**

1. **2D projection** (t-SNE/UMAP): Visualize token similarity space
   - Close tokens = perceptually similar
   - Clusters = musical "gestures" that go together
   - Click any token to see details

2. **Audio playback**: Hear representative examples from training
   - 4-5 exemplar frames per token
   - Shows timestamp and frame ID
   - Can navigate to full training context

3. **Transition graph**: Shows which tokens follow which
   - Probability bars
   - Common chains highlighted
   - Self-loops (token repeats) marked

4. **User annotations**: Human can label tokens with personal meaning
   - Stored in `ai_learning_data/token_annotations.json`
   - Searchable ("show me all 'bluesy' tokens")
   - Displayed in main viewports alongside token number

5. **System analysis**: Automatic characterization
   - Spectral profile (bright, dark, noisy)
   - Temporal envelope (percussive, sustained)
   - Harmonic tendency (consonant, dissonant, ambiguous)

**Learning workflow:**

1. **Exposure phase** (Sessions 1-5):
   - Human sees "Token 42" appear during performance
   - Can click to hear exemplars
   - Starts recognizing the sound

2. **Association phase** (Sessions 6-15):
   - Human adds personal label: "That warm opening sound"
   - Notices Token 42 appears often at phrase beginnings
   - Starts predicting when it will appear

3. **Fluency phase** (Sessions 16+):
   - Human thinks "I'll play something Token 42-ish to trigger that pattern"
   - Can intentionally evoke machine responses by playing sounds that map to specific tokens
   - Develops embodied understanding: Token 42 is not "Cmaj", it's "that particular quality"

### 5.4 Real-Time Decision Reasoning Overlay

**Purpose:** Show machine's reasoning **during performance**, not just in post-hoc logs.

**Implementation:** Integrate `agent/decision_explainer.py` into live GUI

**New Qt signal:**
```python
# In visualization/event_bus.py
decision_reasoning_signal = pyqtSignal(dict)

# Emitted from MusicHal_9000.py after each generation
self.event_bus.decision_reasoning_signal.emit({
    'timestamp': time.time(),
    'mode': current_mode,
    'trigger_event': {...},
    'request': {...},
    'pattern_match': {...},
    'generated': {...},
    'reasoning': explainer.build_reasoning(),
    'confidence': 0.87
})
```

**Display: Slide-in panel** (appears briefly after each decision)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DECISION REASONING                          [ðŸ“Œ Pin] [âœ•]    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 14:23:45 - SHADOW Mode                                      â”‚
â”‚                                                              â”‚
â”‚ Trigger:                                                     â”‚
â”‚   Your input: F4 (MIDI 65)                                  â”‚
â”‚   Token: 142, Consonance: 0.73, Rhythm: [3,2]               â”‚
â”‚                                                              â”‚
â”‚ Request:                                                     â”‚
â”‚   Match gesture_token = 142 (close imitation)               â”‚
â”‚   Target consonance â‰ˆ 0.70 (maintain harmonic quality)      â”‚
â”‚                                                              â”‚
â”‚ Pattern Search:                                             â”‚
â”‚   Filtered 501 states â†’ 23 candidates                       â”‚
â”‚   Best match: State 234 (87% similarity)                    â”‚
â”‚   From training: Bar 47, timestamp 1763676200.45            â”‚
â”‚   [â–¶ Play Training Audio]                                   â”‚
â”‚                                                              â”‚
â”‚ Generation:                                                 â”‚
â”‚   Output: F4 â†’ G4 â†’ A4 â†’ A#4                                â”‚
â”‚   Shape: Ascending, narrow range (5 semitones)              â”‚
â”‚   Duration: 480ms phrase                                    â”‚
â”‚                                                              â”‚
â”‚ Confidence: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 87%                         â”‚
â”‚                                                              â”‚
â”‚ [View Full Trajectory] [Compare with History]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Behavior:**

- **Auto-display**: Slides in for 3 seconds after each decision
- **Dismissible**: Click [âœ•] to close early
- **Pinnable**: Click [ðŸ“Œ] to keep open
- **Minimal mode**: Can collapse to just confidence bar + [Expand] button
- **History**: Can browse last 20 decisions

**Integration:**

```python
# In MusicHal_9000.py, _generate_ai_response()

# After generation
explanation = self.decision_explainer.create_explanation(
    timestamp=time.time(),
    mode=self.behavior_mode,
    trigger_event=event,
    context=recent_context,
    request=request_params,
    pattern_match={'score': match_score, 'state_id': matched_state},
    generated_notes=generated_notes
)

# Emit to GUI
self.event_bus.decision_reasoning_signal.emit(explanation.to_dict())

# Also log to CSV (existing behavior)
self.decision_explainer.log_decision(explanation)
```

### 5.5 Bidirectional Semantic Tagging System

**Purpose:** Let human annotate machine tokens with personal meaning, create shared vocabulary over time.

**Data structure:**

```json
{
  "token_annotations": {
    "42": {
      "labels": ["bluesy opening", "warm C-ish sound", "phrase start"],
      "created": "2025-11-15T14:23:00",
      "last_modified": "2025-11-20T10:45:00",
      "usage_count": 67,
      "context_notes": "Often appears at the beginning of improvised sections"
    },
    "17": {
      "labels": ["descending gesture", "resolution feel"],
      "created": "2025-11-16T09:12:00",
      "usage_count": 43
    }
  },
  "pattern_annotations": {
    "42_17": {
      "label": "My favorite opening move",
      "description": "Token 42 to 17 feels like 'question and answer'",
      "created": "2025-11-18T16:30:00"
    }
  }
}
```

**Storage:** `ai_learning_data/token_annotations.json`

**Interface (in Token Explorer):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TOKEN 42 - Your Annotations                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Labels:                                                      â”‚
â”‚   â€¢ "bluesy opening"        [Edit] [Delete]                 â”‚
â”‚   â€¢ "warm C-ish sound"      [Edit] [Delete]                 â”‚
â”‚   â€¢ "phrase start"          [Edit] [Delete]                 â”‚
â”‚                                                              â”‚
â”‚ [+ Add New Label]                                           â”‚
â”‚                                                              â”‚
â”‚ Context Notes:                                              â”‚
â”‚   "Often appears at the beginning of improvised sections    â”‚
â”‚    Sounds like a comfortable starting point for me"         â”‚
â”‚                                                              â”‚
â”‚   [Edit Notes]                                              â”‚
â”‚                                                              â”‚
â”‚ Usage Statistics:                                           â”‚
â”‚   You've heard this token: 67 times                         â”‚
â”‚   First encountered: Nov 15, 2025                           â”‚
â”‚   Last seen: Today at 14:23                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Display in main viewports:**

```
Audio Analysis:
  Token 42: "bluesy opening" ðŸ·ï¸
  Consonance: 0.73
```

**Machine uses annotations in explanations:**

```
Decision Reasoning:
  Heard your "bluesy opening" gesture (Token 42)
  Responded with descending pattern (Token 17)
  This combination feels like "question and answer" to you
```

**Key feature: Annotations are interpretive, not prescriptive**

- Machine doesn't change its behavior based on annotations
- Annotations help **human understand machine's native categories**
- Over time, builds shared vocabulary ("we both know what 'bluesy opening' means now")

### 5.6 Intentionality Signaling Protocol

**Purpose:** Map existing musical parameters to meta-communicative signals about intent.

**Implementation: Parameter â†’ Intent mapping**

**Dynamics (RMS amplitude) â†’ Attention weight:**

```python
# In listener/jhs_listener_core.py
def extract_intentionality_signals(event):
    """Extract meta-communicative signals from musical parameters"""
    
    # Dynamics â†’ Attention weight
    rms_db = event.rms_db
    if rms_db > -10:      # Very loud
        attention = 1.0   # "Pay close attention to this"
    elif rms_db > -20:    # Loud
        attention = 0.8   # "This is important"
    elif rms_db > -30:    # Medium
        attention = 0.5   # "Normal salience"
    elif rms_db > -40:    # Soft
        attention = 0.3   # "Subtle, feel free to diverge"
    else:                 # Very soft
        attention = 0.1   # "Tentative, don't copy this"
    
    return {'attention_weight': attention}
```

**Applied in agent request:**

```python
# In agent/ai_agent.py
intentionality = listener.get_intentionality_signals()

request = {
    'gesture_token': current_token,
    'consonance': target_consonance,
    # Attention weight modulates gesture token matching
    'gesture_weight': intentionality['attention_weight']  # 0.1 to 1.0
}
```

**Effect:**
- Loud input â†’ Machine matches gesture token closely (high weight)
- Soft input â†’ Machine feels free to diverge (low weight)

---

**Timing variance (IOI standard deviation) â†’ Phrase boundary signal:**

```python
# Track recent IOIs (inter-onset intervals)
recent_iois = [0.25, 0.24, 0.26, 0.25, 0.48]  # Last 5 intervals
ioi_std = np.std(recent_iois)  # 0.095

if ioi_std > 0.1:
    phrase_boundary = True
    # "Human timing is getting irregular, phrase ending likely"
else:
    phrase_boundary = False
    # "Steady timing, phrase continuing"
```

**Rubato (sudden tempo change) â†’ Phrasing signal:**

```python
# Detect rubato
current_ioi = 0.48
avg_ioi = 0.25
if current_ioi > avg_ioi * 1.5:  # 50% slower
    rubato_signal = "ritardando"
    # "Human is slowing down, phrase ending"
elif current_ioi < avg_ioi * 0.7:  # 30% faster
    rubato_signal = "accelerando"
    # "Human is speeding up, building energy"
```

**Applied in mode transitions:**

```python
# In agent/behaviors.py
if intentionality['rubato'] == "ritardando":
    # Don't switch modes mid-ritardando
    # Wait for phrase to complete
    defer_mode_switch = True
```

---

**Timbre (spectral centroid) â†’ Energy matching:**

```python
# Bright timbre â†’ Match with bright generated notes
spectral_centroid = event.centroid  # Hz

if centroid > 3000:
    energy = "bright"
    # Request high-register tokens
elif centroid > 1500:
    energy = "medium"
elif centroid < 1000:
    energy = "dark"
    # Request low-register tokens
```

**Applied in generation:**

```python
request = {
    'gesture_token': current_token,
    'consonance': target_consonance,
    'register': intentionality['energy']  # 'bright', 'medium', 'dark'
}
```

---

**Harmonic noise ratio (HNR) â†’ Timbral matching:**

```python
# High HNR = pure tone (singing, clean guitar)
# Low HNR = noisy (distortion, breath sounds)

hnr = event.hnr  # 0.0 to 1.0

if hnr > 0.7:
    timbre_match = "pure"
    # Generate with clean MIDI notes, legato
elif hnr > 0.4:
    timbre_match = "medium"
elif hnr < 0.3:
    timbre_match = "noisy"
    # Generate with staccato, shorter durations
```

**Applied in MIDI output:**

```python
# In midi_io/midi_output.py
if intentionality['timbre'] == "noisy":
    # Shorter note durations, lower velocity
    duration *= 0.7
    velocity *= 0.8
elif intentionality['timbre'] == "pure":
    # Longer, sustained notes
    duration *= 1.2
```

---

**Summary: Intentionality Parameters**

| Musical Parameter | Intentionality Signal | Effect on Machine |
|-------------------|----------------------|-------------------|
| Loud dynamics (RMS > -20 dB) | "Pay attention" | High gesture token weight (0.8-1.0) |
| Soft dynamics (RMS < -40 dB) | "Tentative" | Low gesture token weight (0.1-0.3) |
| Rubato ritardando (IOI +50%) | "Phrase ending" | Defer mode switches, match timing |
| Rubato accelerando (IOI -30%) | "Building energy" | Increase tempo of generation |
| High spectral centroid (>3000 Hz) | "Bright energy" | Request high-register tokens |
| Low spectral centroid (<1000 Hz) | "Dark mood" | Request low-register tokens |
| High HNR (>0.7) | "Pure tone" | Legato MIDI, longer durations |
| Low HNR (<0.3) | "Noisy texture" | Staccato MIDI, shorter durations |
| IOI variance increasing | "Phrase boundary" | Phrase memory activation |

**Key advantage:** Uses parameters already being extracted, no new analysis needed. Just **reinterprets** them as meta-communicative signals.

### 5.7 Uncertainty Communication (Bidirectional)

**Purpose:** Both human and machine indicate when they "don't understand" or are uncertain.

**Machine â†’ Human uncertainty signals:**

**Visual indicators:**

1. **Pattern match confidence < 60%** â†’ Viewport border flashes yellow
   ```
   â”Œ âš ï¸  LOW CONFIDENCE âš ï¸  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Pattern Match: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 52%                      â”‚
   â”‚ Warning: Weak pattern match, generation may be random   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ```

2. **Ensemble chord disagreement** â†’ Show all candidates
   ```
   Chord: ??? (uncertain)
     â€¢ W2V:   Cmaj  (45%)
     â€¢ Ratio: Cmaj7 (38%)
     â€¢ ML:    Am    (35%)
     â€¢ HC:    C     (28%)
     âš ï¸  No clear winner - ensemble confused
   ```

3. **Low overall confidence < 40%** â†’ Generated notes are transparent/dimmed
   ```
   MIDI Output: [52, 55, 59] â† Displayed with 40% opacity
   "Low confidence generation - may not fit context well"
   ```

**Audio indicators:**

4. **Optional tentative articulation** â†’ When confidence < 40%, play notes with:
   - Shorter durations (50% of normal)
   - Lower velocities (70% of normal)
   - More space between notes
   - Effect: Sounds "hesitant", like machine is "not sure"

**Threshold behavior:**

5. **Critical uncertainty** â†’ If pattern match < 30%, **don't generate**
   ```
   Status: â›” INSUFFICIENT CONTEXT
   "Cannot find matching patterns in training data.
    Waiting for more familiar musical territory..."
   ```

---

**Human â†’ Machine uncertainty signals:**

**Detection:**

1. **Hesitation pattern** â†’ Multiple short phrases with long pauses
   ```
   [Play 2 notes] [Pause 2s] [Play 3 notes] [Pause 3s] [Play 1 note]
   
   Interpretation: Human is uncertain, trying things out
   ```

2. **Repeated gesture** â†’ Same token appearing 3+ times with pauses
   ```
   Token 42 â†’ [Pause] â†’ Token 42 â†’ [Pause] â†’ Token 42
   
   Interpretation: "I'm stuck, help me out"
   ```

3. **Decreasing dynamics** â†’ RMS steadily dropping over 5-10 seconds
   ```
   RMS: -15 dB â†’ -20 dB â†’ -28 dB â†’ -35 dB
   
   Interpretation: Human losing confidence or energy
   ```

**Machine response to human uncertainty:**

```python
# In agent/behaviors.py

def detect_human_uncertainty(recent_events):
    """Detect if human seems uncertain or stuck"""
    
    # Check for hesitation pattern
    pauses = [e.ioi > 2.0 for e in recent_events]
    if sum(pauses) >= 3:
        return {'uncertainty': True, 'type': 'hesitation'}
    
    # Check for repeated gesture
    tokens = [e.gesture_token for e in recent_events]
    if len(set(tokens)) == 1 and len(tokens) >= 3:
        return {'uncertainty': True, 'type': 'stuck'}
    
    # Check for fading dynamics
    rms_values = [e.rms_db for e in recent_events[-5:]]
    if rms_values[0] - rms_values[-1] > 15:  # 15 dB drop
        return {'uncertainty': True, 'type': 'fading'}
    
    return {'uncertainty': False}

# If uncertainty detected:
if uncertainty['type'] == 'stuck':
    # Offer strong suggestion (high confidence pattern)
    # Use phrase memory recall (familiar motif)
    # Switch to COUPLE mode (independent lead)
elif uncertainty['type'] == 'hesitation':
    # Give human space (longer gaps before responding)
    # Simpler patterns (fewer notes, clearer gestures)
elif uncertainty['type'] == 'fading':
    # Energize (louder, brighter, ascending patterns)
    # Try to re-engage
```

**Effect:** Machine adapts behavior when it senses human is uncertain or stuck.

---

## 6. Implementation Roadmap {#implementation-roadmap}

### Phase 1: Dual-Layer Viewports (1 week)

**Goal:** Separate machine perception from human interpretation in GUI.

**Tasks:**

1. **Modify Audio Analysis Viewport** (`visualization/viewports/audio_analysis_viewport.py`):
   - Add "Machine Perception" section (top)
   - Add "Human Interpretation" section (bottom)
   - Add uncertainty indicators (flashing borders, warning icons)
   - Add [ðŸ” Explain] button linking to decision reasoning

2. **Modify Pattern Match Viewport** (`visualization/viewports/pattern_match_viewport.py`):
   - Add provenance display (training bar, timestamp)
   - Add [â–¶ Play Training Audio] button
   - Add request filtering cascade display
   - Add [View Graph] button for trajectory visualization

3. **Add Ensemble Breakdown** to Status Bar viewport:
   - Expandable section showing all 4 chord sources
   - Color-code by confidence
   - Highlight winner, show weighting applied

**Testing:** Run `MusicHal_9000.py`, verify viewports show dual layers, test audio playback buttons.

**Deliverable:** GUI shows both machine-native and human-interpretive information clearly separated.

---

### Phase 2: Token Semantics Explorer (1-2 weeks)

**Goal:** Help human develop fluency with token vocabulary.

**Tasks:**

1. **Create Token Explorer viewport** (`visualization/token_explorer.py`):
   - Implement t-SNE/UMAP projection of 64-token space
   - Load codebook from joblib vocabulary files
   - Interactive click â†’ show token details

2. **Add audio exemplar playback**:
   - Extract audio frames from training JSON (`audio_frames` dict)
   - Index by token (map state â†’ frame â†’ token)
   - Implement [â–¶ Play] button with sounddevice

3. **Transition graph visualization**:
   - Parse AudioOracle transitions from JSON
   - Build tokenâ†’token probability matrix
   - Display as directed graph with Matplotlib/NetworkX

4. **Annotation system**:
   - Create `ai_learning_data/token_annotations.json`
   - Implement [Edit] [Add] UI for labels
   - Save/load annotations persistently

5. **Integrate annotations into main viewports**:
   - Display user labels alongside token numbers: `"Token 42: bluesy opening ðŸ·ï¸"`

**Testing:** Load `General_idea.json` + `General_idea_harmonic_vocab.joblib`, explore token space, add annotations, verify they appear in main GUI.

**Deliverable:** Token Explorer tool that makes machine vocabulary learnable through interaction.

---

### Phase 3: Real-Time Decision Reasoning (1 week)

**Goal:** Show decision explainer output during live performance.

**Tasks:**

1. **Add new Qt signal** to `visualization/event_bus.py`:
   ```python
   decision_reasoning_signal = pyqtSignal(dict)
   ```

2. **Emit signal from** `MusicHal_9000.py` in `_generate_ai_response()`:
   ```python
   explanation = self.decision_explainer.create_explanation(...)
   self.event_bus.decision_reasoning_signal.emit(explanation.to_dict())
   ```

3. **Create Decision Reasoning Viewport** (`visualization/viewports/decision_reasoning_viewport.py`):
   - Slide-in panel (right side of screen)
   - Auto-display for 3 seconds
   - [ðŸ“Œ Pin] and [âœ• Close] buttons
   - [â–¶ Play Training Audio] for provenance
   - History buffer (last 20 decisions)

4. **Connect signal to viewport**:
   ```python
   self.event_bus.decision_reasoning_signal.connect(
       self.decision_viewport.on_reasoning_update
   )
   ```

**Testing:** Run live performance, verify reasoning overlay appears after each decision, test pinning and history navigation.

**Deliverable:** Real-time transparency into machine's reasoning process.

---

### Phase 4: Intentionality Signaling (3-4 days)

**Goal:** Map musical parameters to meta-communicative signals.

**Tasks:**

1. **Extend** `listener/jhs_listener_core.py` with `extract_intentionality_signals()`:
   - Dynamics â†’ attention weight
   - Timing variance â†’ phrase boundary
   - Spectral centroid â†’ energy/register
   - HNR â†’ timbre matching

2. **Modify** `agent/ai_agent.py` to use intentionality in requests:
   ```python
   intent = listener.get_intentionality_signals()
   request = {
       'gesture_token': current_token,
       'gesture_weight': intent['attention_weight'],
       'register': intent['energy'],
       ...
   }
   ```

3. **Add rubato detection** to defer mode switches during phrase endings

4. **Modify MIDI output** (`midi_io/midi_output.py`) to apply timbre matching:
   - Noisy HNR â†’ shorter durations, lower velocity
   - Pure HNR â†’ longer, sustained notes

**Testing:** Play loud vs soft, bright vs dark timbre, rubato vs strict timing, verify machine responds appropriately.

**Deliverable:** Musical parameters carry intentional meaning, machine adapts behavior accordingly.

---

### Phase 5: Uncertainty Communication (3-4 days)

**Goal:** Both entities indicate when uncertain.

**Tasks:**

1. **Add visual uncertainty indicators**:
   - Flashing viewport borders (pattern match < 60%)
   - Transparent/dimmed MIDI output display (confidence < 40%)
   - Ensemble disagreement display (already in Phase 1)

2. **Optional audio uncertainty** (MIDI articulation):
   - Shorter durations when confidence < 40%
   - Lower velocities when uncertain
   - More space between notes

3. **Add human uncertainty detection** in `agent/behaviors.py`:
   - Hesitation pattern detector
   - Repeated gesture ("stuck") detector
   - Fading dynamics detector

4. **Implement adaptive response** to human uncertainty:
   - Stuck â†’ COUPLE mode + phrase memory recall
   - Hesitation â†’ Simpler patterns, longer gaps
   - Fading â†’ Energizing patterns (ascending, louder)

**Testing:** Test low-confidence scenarios, verify visual indicators. Intentionally play "stuck" patterns, verify machine switches to leading mode.

**Deliverable:** Uncertainty acknowledged and communicated bidirectionally.

---

### Phase 6: Research Validation (Ongoing)

**Goal:** Test if these communication bridges actually work in practice.

**Experiments:**

1. **Token fluency development** (4-6 weeks):
   - Track user annotations over time
   - Measure token recognition accuracy (can user predict which token?)
   - Compare early sessions (week 1) to later sessions (week 6)

2. **Transparency impact on trust** (qualitative):
   - Self-report trust ratings before/after dual-layer viewports
   - Measure engagement (flow state indicators)
   - Record subjective experience

3. **Intentionality signaling effectiveness**:
   - Test: Does loud input â†’ closer imitation?
   - Test: Does rubato â†’ delayed mode switches?
   - Test: Does timbre matching improve musical coherence?

4. **Uncertainty detection accuracy**:
   - Test: Can machine detect when human is stuck?
   - Test: Do visual uncertainty cues help human decision-making?

**Deliverable:** Research documentation of communication bridge effectiveness.

---

## 7. Research Questions and Open Problems {#research-questions}

### 7.1 Token Fluency Questions

**Q1: Can humans develop genuine fluency with 64-token vocabulary?**

**Hypothesis:** After 20-30 hours of exposure with token explorer tool, users can:
- Recognize tokens by sound (>70% accuracy)
- Predict token transitions (>60% accuracy for common patterns)
- Intentionally evoke specific tokens by playing sounds that map to them

**Test:** Controlled experiment:
- Week 0: Baseline token recognition (expect ~15% random chance for 64 tokens)
- Weeks 1-6: Regular practice with token explorer
- Week 6: Post-test token recognition (expect >70% for common tokens)

**Alternative outcome:** Users develop partial fluency (recognize 10-20 common tokens but not full vocabulary).

**Implication for system:** If full fluency is impossible, might need to reduce vocabulary size (64 â†’ 32 â†’ 16?) or cluster tokens into semantic categories ("bright tokens", "dark tokens", etc.).

---

**Q2: Do user annotations converge with acoustic properties, or diverge into personal meanings?**

**Hypothesis:** Annotations will show **both** convergence and divergence:
- Convergent: "Token 42 = bright sound" (matches high spectral centroid)
- Divergent: "Token 42 = my opening gesture" (personal performance habit, not acoustic)

**Test:** Analyze annotation semantics:
- Extract acoustic features for each token (avg spectral centroid, HNR, consonance)
- Perform sentiment/semantic analysis of user labels
- Measure correlation: Do "bright" labels correlate with high centroid?

**Implication:** Annotations are **co-created meaning**, not pure translation. User and machine build shared vocabulary through use.

---

**Q3: Does token vocabulary remain stable across training corpora, or is it corpus-specific?**

**Known issue:** Token 42 in `General_idea.json` might represent different sound than Token 42 in `Curious_child.json`.

**Hypothesis:** Tokens are **corpus-specific** because:
- Each training creates new codebook (fresh KMeans clustering)
- Different musical styles â†’ different cluster centers
- Token numbers are arbitrary (no semantic alignment across trainings)

**Test:**
- Train on 3 different musical styles (jazz, classical, electronic)
- Compare codebook cluster centers (768D embeddings)
- Measure centroid distance between "Token 42" across trainings

**Implication:** 
- If stable: Could build universal token semantics ("Token 42 always sounds like X")
- If unstable: Need per-corpus token learning (user re-learns tokens for each training)

**Possible solution:** Align codebooks across trainings using Procrustes analysis or transfer learning.

---

### 7.2 Transparency vs Magic Questions

**Q4: Does transparency increase trust or decrease engagement?**

**Tension:** Seeing "Pattern match 87% to training bar 47" might:
- **Increase trust** (understanding the mechanism)
- **Decrease engagement** (breaks the magic, feels mechanical)

**Hypothesis:** U-shaped curve:
- Low transparency â†’ Low trust (system feels random)
- Medium transparency â†’ High trust + engagement (sweet spot)
- High transparency â†’ High trust but low engagement (too mechanical, no mystery)

**Test:** Three conditions:
- **Minimal:** Only show mode, chord, MIDI output
- **Medium:** Add pattern match scores, token numbers, reasoning (proposed system)
- **Maximal:** Show full AudioOracle graph, feature vectors, probability distributions

Measure:
- Trust (self-report scale 1-10)
- Engagement (flow state questionnaire)
- Musical quality (external judges blind rating)

**Implication:** Might need **adjustable transparency** (user can toggle verbosity).

---

**Q5: Do musicians prefer machine-native transparency or symbolic translation?**

**Question:** Even with dual-layer viewports, which layer do users actually look at?

**Hypothesis:** 
- **Early sessions:** Users rely on symbolic layer ("Cmaj" familiar)
- **Later sessions:** Users transition to machine layer ("Token 42" more meaningful)

**Test:** Eye-tracking study or explicit reporting:
- "Which information did you use to make your next decision?"
- Track viewport interaction (which buttons clicked most)

**Possible outcomes:**
- Users always prefer symbolic (contradicts our theory)
- Users transition to native (validates embodied learning hypothesis)
- Users use both (hybrid strategy)

---

### 7.3 Intentionality Signaling Questions

**Q6: Can dynamics/timing/timbre reliably signal intent, or is context needed?**

**Problem:** Same loud dynamics might mean:
- "This is important" (emphasis)
- "I'm playing forte naturally" (just how I play)
- "Testing if mic is working" (not musical signal)

**Hypothesis:** Context-dependent interpretation required:
- Sudden loudness change (Â±10 dB in 2s) â†’ Intentional signal
- Gradual/consistent loudness â†’ Natural playing style
- Need baseline calibration per user

**Test:** Calibration session:
- "Play normally for 60 seconds" â†’ Establish RMS baseline
- "Play something important" â†’ Measure RMS delta
- Use deltas (not absolutes) for intentionality

**Implication:** Need per-user calibration, not universal thresholds.

---

**Q7: Do users consciously use intentionality signals, or are they implicit?**

**Question:** Will users think "I'll play loud to make it copy me" or is it unconscious?

**Hypothesis:** Initially implicit (natural playing), becomes conscious with feedback:
- Early sessions: Users don't think about dynamics/timbre as signals
- After discovering correlation: "Oh, when I play loud it matches more closely"
- Later sessions: Conscious use ("I'll play soft to let it diverge")

**Test:** Think-aloud protocol:
- Ask users to verbalize decisions during performance
- Early sessions: Expect no mention of dynamics-as-signal
- Later sessions: Expect explicit strategizing

**Implication:** System enables **new musical behaviors** (meta-communication through parameters).

---

### 7.4 Uncertainty Communication Questions

**Q8: Do visual uncertainty indicators help or distract?**

**Concern:** Flashing borders, transparency, warning icons might be:
- Helpful (user knows machine is uncertain)
- Distracting (breaks flow, draws attention to errors)

**Hypothesis:** Depends on user preference and musical context:
- Analytical users (want to understand) â†’ Helpful
- Intuitive users (flow-focused) â†’ Distracting
- Complex music (lots of changes) â†’ Helpful
- Meditative music (long sustained tones) â†’ Distracting

**Test:** A/B comparison:
- Half of performances with indicators
- Half without
- Measure: Trust ratings, flow state, musical quality

**Implication:** Might need **toggleable uncertainty display** (user preference).

---

**Q9: Can machine accurately detect human uncertainty?**

**Validation needed:** Do hesitation patterns, repeated gestures, fading dynamics actually correlate with human's subjective uncertainty?

**Test:** Video + self-report:
- Record performances with webcam
- After each pause, ask: "Were you uncertain there? (Yes/No)"
- Compare machine's uncertainty detection to human's self-report
- Calculate precision, recall, F1 score

**Possible finding:** False positives (machine thinks human is uncertain when they're not) might be acceptable if system response is non-intrusive (just offering suggestions, not forcing changes).

---

## 8. Technical Specifications {#technical-specifications}

### 8.1 Data Flow Modifications

**Current data flow:**
```
Audio â†’ Event â†’ Perception â†’ Request â†’ AudioOracle â†’ Generation â†’ MIDI
                                 â†“
                        Ensemble Voting â†’ Display "Cmaj"
```

**Proposed data flow:**
```
Audio â†’ Event â†’ Perception â†’ Intentionality Extraction
                      â†“              â†“
                 [Token 42]  [Attention: 0.8, Energy: bright]
                      â†“              â†“
                 Request (machine-native + intent-modified)
                      â†“
                 AudioOracle Query
                      â†“
                 Pattern Match + Provenance
                      â†“
                 Decision Explainer
                      â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â†“                     â†“
    MIDI Output          Event Bus Signals
          â†“                     â†“
    Sound Output      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â†“                           â†“
              Viewports (dual-layer)    Reasoning Overlay
                      â†“
            Token Explorer (on-demand)
```

**Key changes:**
1. **Intentionality extraction** as parallel stream
2. **Provenance tracking** (which training frame matched)
3. **Decision explainer** emits to event bus (not just CSV logs)
4. **Multiple visualization pathways** (viewports, overlay, explorer)

### 8.2 File Structure Additions

**New files:**

```
visualization/
  â”œâ”€â”€ token_explorer.py              # NEW: Token semantics tool
  â”œâ”€â”€ viewports/
  â”‚   â”œâ”€â”€ decision_reasoning_viewport.py  # NEW: Real-time reasoning
  â”‚   â”œâ”€â”€ audio_analysis_viewport.py      # MODIFIED: Dual-layer
  â”‚   â”œâ”€â”€ pattern_match_viewport.py       # MODIFIED: Add provenance
  â”‚   â””â”€â”€ request_params_viewport.py      # MODIFIED: Intentionality display
  â””â”€â”€ event_bus.py                    # MODIFIED: Add decision_reasoning_signal

listener/
  â””â”€â”€ intentionality_extractor.py     # NEW: Extract meta-comm signals

ai_learning_data/
  â””â”€â”€ token_annotations.json          # NEW: User annotations

analysis/
  â”œâ”€â”€ token_space_projector.py        # NEW: t-SNE/UMAP projection
  â””â”€â”€ provenance_tracker.py           # NEW: Map states â†’ training audio
```

### 8.3 Performance Requirements

**Latency targets:**

| Component | Current | Target | Max Acceptable |
|-----------|---------|--------|----------------|
| Audio â†’ Event | 11.6ms | 11.6ms | 15ms |
| Perception (MERT) | 30-50ms | 30-50ms | 60ms |
| Intentionality extraction | N/A | 5ms | 10ms |
| AudioOracle query | 5-20ms | 5-20ms | 30ms |
| Decision explainer | N/A | 10ms | 20ms |
| GUI update (viewports) | 30-100ms | 30-100ms | 150ms |
| **Total end-to-end** | **<100ms** | **<100ms** | **150ms** |

**Memory requirements:**

| Data Structure | Current | With Changes | Notes |
|----------------|---------|--------------|-------|
| AudioOracle graph | ~50MB | ~50MB | Unchanged |
| Audio frames | Embedded in JSON | Extract on-demand | 500 Ã— 2s @ 44.1kHz = ~45MB |
| Token annotations | N/A | ~100KB | Text labels, small |
| t-SNE projection | N/A (runtime) | Cache ~500KB | 64 tokens Ã— 2D coords |
| Decision history | ~1MB (CSV logs) | +500KB (in-memory) | Last 100 decisions |

**Total memory overhead:** ~55MB (acceptable for modern systems)

### 8.4 Qt Signal Specifications

**New signals:**

```python
# In visualization/event_bus.py

class EventBus(QObject):
    # Existing signals
    pattern_match_signal = pyqtSignal(dict)
    mode_change_signal = pyqtSignal(dict)
    audio_analysis_signal = pyqtSignal(dict)
    # ...
    
    # NEW SIGNALS
    decision_reasoning_signal = pyqtSignal(dict)
    # Payload: {
    #     'timestamp': float,
    #     'mode': str,
    #     'trigger': {...},
    #     'request': {...},
    #     'pattern_match': {...},
    #     'generated': {...},
    #     'reasoning': str,
    #     'confidence': float
    # }
    
    intentionality_signal = pyqtSignal(dict)
    # Payload: {
    #     'attention_weight': float (0.0-1.0),
    #     'energy': str ('bright', 'medium', 'dark'),
    #     'timbre': str ('pure', 'noisy'),
    #     'phrase_boundary': bool,
    #     'rubato': str ('none', 'ritardando', 'accelerando')
    # }
    
    uncertainty_signal = pyqtSignal(dict)
    # Payload: {
    #     'pattern_match_confidence': float,
    #     'ensemble_agreement': bool,
    #     'human_uncertainty_detected': bool,
    #     'type': str ('hesitation', 'stuck', 'fading', None)
    # }
    
    token_annotation_updated = pyqtSignal(int, dict)
    # Payload: token_id (int), annotation_data (dict)
```

### 8.5 JSON Schema Additions

**Token annotations file** (`ai_learning_data/token_annotations.json`):

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "token_annotations": {
      "type": "object",
      "patternProperties": {
        "^[0-9]+$": {
          "type": "object",
          "properties": {
            "labels": {
              "type": "array",
              "items": {"type": "string"}
            },
            "created": {"type": "string", "format": "date-time"},
            "last_modified": {"type": "string", "format": "date-time"},
            "usage_count": {"type": "integer"},
            "context_notes": {"type": "string"}
          }
        }
      }
    },
    "pattern_annotations": {
      "type": "object",
      "patternProperties": {
        "^[0-9]+_[0-9]+$": {
          "type": "object",
          "properties": {
            "label": {"type": "string"},
            "description": {"type": "string"},
            "created": {"type": "string", "format": "date-time"}
          }
        }
      }
    },
    "metadata": {
      "type": "object",
      "properties": {
        "vocabulary_source": {"type": "string"},
        "created": {"type": "string", "format": "date-time"},
        "total_tokens": {"type": "integer"}
      }
    }
  }
}
```

---

## Summary

This document outlines a comprehensive plan for building **communication bridges across incommensurable perceptual substrates** in the MusicHal 9000 system.

**Core principles:**
1. No forced translation - show both substrates honestly
2. Embodied learning - help human develop token fluency
3. Intentionality through music - dynamics/timing/timbre carry meaning
4. Uncertainty acknowledged - both entities indicate "I don't know"
5. Shared vocabulary emerges - annotations co-created over time

**Implementation phases:**
1. Dual-layer viewports (1 week)
2. Token explorer tool (1-2 weeks)
3. Real-time reasoning overlay (1 week)
4. Intentionality signaling (3-4 days)
5. Uncertainty communication (3-4 days)
6. Research validation (ongoing)

**Expected outcome:** Human and machine develop **mutual understanding** through accumulated interaction, without either learning to "speak" the other's native language. Communication happens **across** the substrate gap, not by eliminating it.

**Next steps:** Tomorrow, decide:
- Which phase to implement first?
- Do we test with current General_idea.json model?
- What's the priority: transparency (viewports) or learning (token explorer)?

---

**End of document**  
**Word count:** ~15,000  
**Created:** 21 November 2025  
**Ready for implementation discussion**
