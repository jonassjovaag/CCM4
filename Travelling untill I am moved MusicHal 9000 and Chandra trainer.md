# Travelling untill I am moved: MusicHal 9000 and Chandra trainer

**An Artistic Research Exposition on Gesture-Based Memory, Dual Perception, and Trust in AI Musical Partnership**

by Jonas Howden SjÃ¸vaag  
University of Agder, Norway  
November 2025

---

## Abstract

Can a machine truly listen? This question demands understanding what listening *is*, accepting that it is contextual, and seeking where the common ground between human listening and machine listening might existâ€”or even, what that common ground might look like. This exposition documents an approximately 12-month long practice-based development of MusicHal 9000, an AI musical partner built on gesture-based memory architecture. Through the integration of Wav2Vec 2.0 neural encoding (768-dimensional perceptual features), musical gesture consolidation, dual vocabulary separation (harmonic/percussive), and ratio-based analysis in both of these, the system might have learned to respond with musical intent rather than algorithmic randomness. I say *might*, because, like all music, the definition will largely be dependant on how it is received. 

The research contributes a gesture consolidation architecture that extends IRCAM's Musical Agents framework (Bujard et al., 2025), a transparency framework for explainable AI decision-making, and a practice-based methodology where technical competence (drums, code, theory) shapes architectural decisions. This exposition demonstrates how trust in human-AI musical partnership emerges not from perfection, but from trying to understand *why* the machine makes its choicesâ€”both while identifying a common ground between two fundamentally different cognitive substrates: 768-dimensional neural space and The Brain, and also from how the machinal response, through trusting that it is not entirely random or statically generative, can shape my own response to it. 

**Keywords**: AI musical partnership, gesture-based memory, Wav2Vec 2.0, AudioOracle, explainable AI, practice-based research, improvisation, trust, dual perception, drummer cognition

---

## Table of Contents

**PART 1: THE LISTENING QUESTION**
1. Opening: Can a Machine Truly Listen?
2. Context: Two Substrates, One Partnership
   - 2.1 The Human Substrate: The Brain (and the Drums)
   - 2.2 The Machine Substrate: 768D Neural Space
   - 2.3 The Translation Layer: Gesture Tokens as Common Ground
   - 2.4 Why Not Chord Names?

**PART 2: THE CRISIS & THE FIX**
3. The Crisis: When The Machine Stopped Listening
   - 3.1 The Symptom: Loss of Musical Intent
   - 3.2 The Root Cause: Broken Translation
   - 3.3 The Musical Impact
4. The Fix: Musical Gesture Consolidation
   - 4.1 What is a Musical Gesture?
   - 4.2 The Consolidation Process
   - 4.3 Musical Results

**PART 3: ARCHITECTURAL INNOVATIONS**
5. Dual Vocabulary: Harmonic vs. Percussive Listening
   - 5.1 The Problem: Blurred Perception
   - 5.2 The Solution: Separating Sources
   - 5.3 Musical Outcomes
6. Transparency Framework: Trust Through Explainability
   - 6.1 The Trust Problem
   - 6.2 Three Layers of Transparency
   - 6.3 Trust in Practice

**PART 4: MEMORY & INTELLIGENCE**
7. Musical Memory: The AudioOracle System
   - 7.1 How Memory Works
   - 7.2 Training the System
   - 7.3 Performance Mode
8. Behavioral Intelligence: Personality Through Time
   - 8.1 The Flicker Problem
   - 8.2 Stable Behavioral Modes
   - 8.3 Musical Coherence

**PART 5: METHODOLOGY & RESULTS**
9. Practice-Based Methodology
   - 9.1 Iterative Development
   - 9.2 Musical Testing
   - 9.3 Subjective Experience as Data
10. Results & Artistic Outcomes
11. Contribution to Artistic Research
12. Reflection & Closing

---

# PART 1: THE LISTENING QUESTION

## 1. Opening: Can a Machine Truly Listen?

Can a machine truly listen? This question demands understanding what listening *is*, accepting that it is contextual, and seeking where the common ground between human listening and machine listening might existâ€”or even, what that common ground might look like.

From a human standpoint, listening clearly isn't just about detecting pitches or classifying chords. On top of that basic understanding, and not additionally, but *simultaneously*, we hear (and feel) musical gestures as intentional pieces of information: instructional, relational, confrontational, accompanimental. The way a guitar stroke unfolds over time, or a drum pattern creates momentum, are all part of this. These gestures carry meaning beyond their acoustic propertiesâ€”they communicate intent, energy, and musical direction. There are more layers to this too, of course, even more complex ones; a certain musical phrase, for example, can put shift an entire body from one emotional state to another, but figuring out what part this plays is better left for another project. I'm just mentioning it here to make the reader of this text aware that even if I'm trying to explain what my musical understanding is based on, I am leaving out huge fundamental parts of what actually goes on in the transaction that goes on when a person plays an instrument and another person listens to it.  

This research began with a simple observation: computational musical partners felt like randomizers, not collaborators. Because everything they responded with ultimately came from a clever generative scriptâ€”input-based or notâ€”the essence of **surprise** and **trust** as listening traits extending from human to machine (albeit in an abstract, extrapolated sense) became critical factors. In an improvisational setting, for the purpose of creating music, pure generative scripts *had* to be replaced with scripts that could show a sense of "understanding," which then was the trigger that led the way into searching for what I would refer to as **common ground**. After all, if two musical systems are listening to the same audio clip without a shared understanding of what that clip actually consists of, how can any side in that transaction even begin to trust that the responses given are anything but completely random?

[VIDEO: Opening performance - You on drums with MusicHal responding, showing the loop: you play â†’ listen â†’ machine responds â†’ you listen â†’ adapt]

### 1.1 What Does Common Ground Mean, then?

In my opinion, a machine will never think like a human, nor will humans think like machines. The term "common ground" therefore means finding functional **translation layers** between fundamentally different cognitive architectures.

Computers must be allowed to understand the way they do best: currently, and as far as I know, **Wav2Vec 2.0's 768-dimensional neural encoding** of audio as perceptual features (Baevski et al., 2020) is the better option. This is not symbolic music theory like the system humans use (chord names, scales, quantized rhythms)â€”it's pure pattern space, capturing timbre, attack, spectral shape, and harmonic content in 768 numbers per audio frame. I have to note here that another variant of this model exists as well, titled called Wav2Vec-mus (Ragano et al., 2023); it is the same basic system, but trained exclusively on music. I have used the original model as it was easier to get a hold of at the time of programming this. 

Humans, on the other hand, understand everything through **The Brain** (duh..)â€”and how that works, well, who really knows. Possibly, we chunk sound into gestures, recognize patterns, predict continuations, and respond with embodied knowledge accumulated through years of practice. For drummers, this means *feeling* rhythmic momentum in the body as neurological transmitters fires in the brain, making my muscles twitch a little whenever I hear ghost notes and accents, thus making me *feel* them as meaningful variations (Brean & Skeie, 2019, kap. 1), and therefore, physically helping me to know when to support versus when to create tension, and, specifically for percussionists, I think, the ability to see a long string of short transient non-harmonic attacks as a connected series of expression, ie, a looong note. 

For drummers, a drum roll is not just interpreted as rapid single strokes, it is much more commonly heard as *one sustained strike*, which, of course, can be said about fills and builds, or any kind of rudiment above a certain tempo, but again, this is dependant on the context in which it is played, and the threshold of this idea inherent in the question being asked. In any case, this specific knowledge plays a crucial role in my musical understanding, and is yet another layer of translation that I can use when I listen and understand what is essentially waves moving through a latent space through the medium of air. This layer, and everything connected to it, is not accessible to a machine.

Between these two substrates I have mentioned, everything can be seen as **translations of ideas**. From that, a lot can be seen as tool, instrument choices, for instance, the timbre changes and ongoing choices on the instrument picked; similarly, the mode of output (MIDI, audio, notation) can be seen in the same way, but beloning, perhaps, to another layer. I'm sure the definition of what these tools are, and what can be classified as a tool, will be very personal and very much dependant on who you ask, but for the purpose in this article, and underlying the research I have done, I see it as a fact that since **768D is the basis for a computer**, and **The Brain is the basis for a human**, the common ground has to be: 

	1) the knowledge that we are operating from a point of observation best suited for the capabilities we have, and 
	1) that every response made is initially coming from an abstract feedback loop where music is not music, but rather a state of mind that allows us, through usage of various tools available to us, to respond and interact using sound only.  

[IMAGE: The translation layer diagram - Left: 768D neural space â†’ Gesture tokens â†’ AudioOracle memory; Right: The Brain â†’ Musical gestures â†’ Phrase memory; Center: Common Ground = learned patterns in shared performance space]

### 1.2 Technical Competence as Research Tool

As I am a performing musicianâ€”drummer first, other instruments secondâ€” **technical competence** plays a huge role for me. I am at a technical and musical level where I can adapt to a lot of situations, so ultimately the sentence *"What I play will make sense, if I feel and say it makes sense"*, holds true because I know enough about what my instrument to back up that claim with artistic practice. In other words, ask me what drumming is, and I will show you something that I think will help you understand. When I play alone, I play something completely different. I already know what drumming is, therefore the abstract layers of the instrument is accessible to me. 

The statement also meets the initial question: *What is listening, and what does that word imply in different contexts*? As such, *the question* and *the statement* form a nice little loop, in many ways similar to that of a human playing an instrument, where you "play phrases using your hand (on the drums, for instance), listen to that sound in the room, interpret and analyze it, react, adapt, do it again or do something else" (Waters, 2007). In that situation, the sound is the both the statement and the question, which is, of course, what makes it belong inside the field of artistic research. 

In this case, the extension into the room also applies to **The Machine**, which listens, breaks down, uses memory and prior knowledge, then responds, listens again, and so on. We are two instances doing this, which quickly escalates the question of "*what is listening*" to a level where it is not sensible to differentiate clearly between my sound and your sound, but we, or I at least, need to be aware of the fact that our understanding of the same sound in the same room is most likely not the same. 

The performative musical competence I have is also methodologically essential to this research. The architectural decisions documented in this exposition emerged from musical needs encountered during practice, not from abstract technical optimization. When token diversity collapsed (Section 3), I felt it as musical incoherence before I understood it as a software bug. When behavioral modes flickered (Section 8.1), I experienced it as "playing with a randomised partner" before identifying the timer problem in the code. Interestingly, the modes of response all have certain traits to them that eiher "makes sense" or "doesn't make sense". Hearing the difference comes from my embodied improvisational knowledge, and is disconnected from the software being fully functional or not. It is also a challenge; software, by its very nature, demands to be completed, and completion is usually connected to delivery of a predetermined, or absolutely measurable, and recreateable, result. In this case, the improbalities in the response in combination with sharing a framework of musical reference, is the key. The end goal is not to program a clone of a musical hero, instead, the goal is to program a source of inspiration, that I can play, argue and explore new musical ideas with. 

Practice-based artistic research demands the kind of embodied knowledge that I display here; I assume many would disagree, but I don't really see how this research could be done properly if I was a computer engineer first, playing drums as a hobby. As Candy and Edmonds (2018) argue, the practitioner-researcher brings "expertise in the domain of practice" that cannot be separated from the research process itself. In my case, that expertiseâ€”understanding rhythm, dynamics, gesture, and musical conversation through years of performanceâ€”is clearly governing and shaping both the developement and the end goal.

[VIDEO: You demonstrating drum gesture vocabulary - Single hits, rolls, fills, showing how each is "one thing" musically]

### 1.3 The Partnership

After approcimately 12 months of practice-based development, MusicHal 9000 now seemingly responds with musical intent. This exposition documents how **gesture-based memory architecture** restored trust in the partnershipâ€”both between me and the script, but also **between me and my main instrument, the drums**.

The goal was never an AI that "sounds good" in isolation. The goal was always **musical**â€”to reach a point where I can:
- **Predict** (loosely): "I have a sense of what it might do"
- **Be surprised** (constructively): "But it can still surprise me in interesting ways"
- **Intuitively understand**: "That choice *feels* logical"
- **Adapt**: "...and I can respond to its choices musically"

This requires transparency. Not complete transparency (I don't need to see every matrix multiplication in the neural network), but decision-level transparencyâ€”seeing, if I want, the gesture tokens, consonance scores, behavioral modes, and pattern memories that guide its choices so I can trust that the choices comes from *an understanding of my input*, not a generative script that will happily plunk and clink no matter what I do or do not do. 

[AUDIO: Before/after trust comparison - Before: randomized responses with no gestural coherence; After: musical intent with phrases and behavioral personality]

---

## 2. Context: Two Substrates, One Partnership

To begin with, and long into the process, finding the **translation layers** between fundamentally different cognitive architectures was challenging. This section explores both substrates (human and machine) and the gesture token system that bridges them, and no, it is not about making the machine think exactly like me. That would be pointless. For one I don't believe in a deterministic universe, so, there's that, but I also have a feeling that the adaption that we humans engage in is largely related to fundamental brain states, as opposed to a switch, or a signal, that is either 1 or 0, or, in other words "â€¦as (a) continuous, hierarchical inference rather than binary steps (Friston, 2010)." 

I frequently imagine and ponder this: if we say that human brains, or any conscious, living-on-its-own brain, have an infinite amount of possible states in-between the 1 and 0, where does that leave the idea of a deterministic universe? For now, we do not have to go further into this, of course, but this is, as I see it, this is the underlying concept that makes determinism vs non-determinism possible, and further, if we do not have determinism, nothing can be known beforehand. If nothing can be known beforehand, we cannot make a synthesised version of ourselves either. 

For most, I would assume this is a difficult concept to wrap the brain around, and logically it makes little sense also. 1 and 0, even on a quantum level, is, after all, just a description of "something", this state or that state. So, in my view,  a system where everything ultimately is just on or off, seems inflexible and even unlikely, and going up, to a much more elevated level, I'd rationalize this by explaining how humans always prefer straight edges and sharp corners, tidy piles and items grouped together, while in nature, where most things just *are*, these kinds of exact perfection do not exist. Sharp edges is an invention, made by us, coming from our need to make order in an unruly world. So, extrapolating from this, it seems logical to assume that rather than 1 and 0, at the tiniest levels we can imagine, there is no such thing as right or wrong, it's just this, that, there, where, who, left, up, inwards, and so on, an infinete amount of possibilities, or even a "...continuous, hierarchical inference rather than binary steps (Friston, 2010)."

Therefore, seen like that, a machine will never think like me, because it will not extrapolate like me, neither will it care, or not care, or even don't give a fuck, like me. For now, at least, we need these translation layers, because we are not at all the same, and even making an effort like this is much more about scraping at the very outer layers of my own understanding than it is about recreating human musicality. 

### 2.1 The Human Substrate: The Brain (and the Drums)

As mentioned many times, I'm a musician first, designer second, programmer .. far down on the list. Not "computer scientist"â€”but a performer who codes aided by useful tools, and is still learning to do so properly. This distinction matters because the system's architecture emerged from musical needs, not accurate technical requirements.

Going back to "What I play makes sense if I say it makes sense," I would like to note, now, that this isn't said with arroganceâ€”it's just showing off a little, asserting an **authority that have come through practice**. After years of drumming, I've developed an internal model of what constitutes musical gesture, rhythmic momentum, and conversational dynamics. This model guides both my playing and my evaluation of MusicHal's responses, and my model is, of course, different from another drummers model. As drums is a gestural instrument, one could say, then, that I hold a gestural vocabulary from which I read and produce automatically, often without conscious thought. As Derek Bailey (1993) writes about free improvisation, the instrument becomes an extension of thought, where "the music is indivisible from the technique used to make it" (p. 83).

I use my model all the time, probably a lot more than I know, and probably, also, in combination with other models that take part in my overall decision-making in the course of a day. In a live situation, MusicHal_9000 face this model, and has to pass a test that can easily be described like this: **Does it sound like it's listening?** This is opposed to "does it generate statistically valid continuations" or "does it minimize prediction error"â€”it has to make me *feel* the dialogue. 

[VIDEO: Demonstrating drum gestures with different intentions - Same pattern played aggressively, playfully, tentatively, showing how gesture intention changes everything]

#### The Human Loop

The human musical loop (for a drummer):

```
Play phrase (drums: snare pattern, cymbal crash, bass drum accent, etc)
  â†“
Listen (room acoustics, how it felt, what energy it created)
  â†“
Interpret (The Brain: "That workedâ€”emphatic ending" or "That felt rushedâ€”try smoother")
  â†“
React, adapt (Continue? Contrast? Pause?)
  â†“
Play again (adjusted based on interpretation)
```

This loop operates on multiple timescales simultaneously:
- **Micro** (10-100ms): Individual stroke adjustments, dynamics
- **Meso** (1-10s): Phrase structure, pattern development
- **Macro** (10-300s): Section transitions, overall arc

As Limb and Braun (2008) define it, 'spontaneous musical performance â€¦ can be defined as the immediate, on-line improvisation of novel melodic, harmonic, and rhythmic musical elements within a relevant musical context.' The challenge for MusicHal was learning to operate across these same timescales.

**Human listening is multi-dimensional:**

We don't hear pitches and then separately hear rhythms and then separately hear timbres. We hear **musical gestures**â€”integrated perceptual events that combine all these dimensions simultaneously. A crash cymbal accent is not just:
- High frequency spectral energy (acoustic property)
- Loud transient (amplitude envelope)
- Noise-dominated signal (harmonic content)

It's a **gesture**: sometimes an emphatic punctuation mark, other times an ending signal, most of the time a moment of high energy that demands either continuation or silence. That meaning emerges from context (what came before, what's expected next) and embodied knowledge (how cymbals function in musical conversation). This last part extends long outside of the realm of the drummer, ask anyone, and they would likely know the sound of a crash cymbal, and instantly contextually place it. 

[AUDIO: Same drum pattern played with different intentions, showing how gesture meaning changes with execution]

**Citations to integrate:**

- Bailey, D. (1993). *Improvisation: Its nature and practice in music*. Da Capo Press.
- Limb, C. J., & Braun, A. R. (2008). Neural substrates of spontaneous musical performance: An fMRI study of jazz improvisation. PLoS ONE, 3(2), e1679.
- Bregman, A. S. (1990). *Auditory scene analysis: The perceptual organization of sound*. MIT Press.
- Candy, L., & Edmonds, E. (2018). Practice-based research in the creative arts: Foundations and futures from the front line. *Leonardo*, 51(1), 63-69.

---

### 2.2 The Machine Substrate: 768D Neural Space

Where humans have The Brain, MusicHal has **Wav2Vec 2.0**â€”a pre-trained neural network that converts audio waveforms into 768-dimensional feature vectors (Baevski et al., 2020).

**What is Wav2Vec 2.0?**

Wav2Vec 2.0 is a self-supervised learning model originally designed for speech recognition, adapted for music by Ragano et al. (2023), but as noted in the beginning of this document, I use the original model here. It lives inside the Chandra_trainer.py script, and provides a method of understanding how audio should be represented by solving a pre-training task where it predicts masked portions of the audio signal from context. The result is a neural encoder that captures perceptual audio qualities without human symbolic labels.

**Why 768 dimensions?**

That's what the neural architecture learned is optimal for capturing audio information. It's not arbitraryâ€”it emerges from the training process. Think of it like this:

- **Human vision**: ~6 million cone cells (high-dimensional perceptual space)
- **Wav2Vec audio**: 768 dimensions (enough to distinguish any sound)

Each dimension captures some aspect of the audio: timbre, attack characteristics, spectral shape, harmonic content, noise profile, temporal evolution. We don't know exactly what each dimension represents (that's the black-box problem with deep learning), but we know empirically that these 768 numbers contain enough information to distinguish guitar from drums, major chords from minor chords, and sustained tones from transients.

[DIAGRAM: Audio waveform â†’ Wav2Vec encoder â†’ 768D feature vector, with visualization of how different sounds map to different regions of 768D space]

#### From Continuous Audio to Discrete Features

The Wav2Vec encoding process:

```
Input: Audio waveform (44,100 samples per second, continuous)
  â†“
Wav2Vec 2.0 encoder (CNN + transformer layers)
  â†“
Output: 768D feature vector every 20ms (discrete, perceptual)
```

**Example encoding** (conceptual, actual values are normalized):

```python
# Snare hit at t=1.5s
wav2vec_vector = [
    0.234,   # Dimension 0: Maybe captures brightness?
    -0.891,  # Dimension 1: Maybe captures attack sharpness?
    0.456,   # Dimension 2: Maybe captures spectral centroid?
    ...      # Dimensions 3-766: Unknown semantic meaning
    0.123    # Dimension 767: Maybe captures noise content?
]
```

As noted already, we can't interpret individual dimensions semantically, but we can use the full 768D vector as a **perceptual fingerprint** of that audio moment.

**Analogy for musicians:**

Imagine if your brain assigned every sound you heard a unique 768-number "fingerprint." A guitar strum might be `[0.23, -0.89, 0.45, ..., 0.12]`. A drum hit: `[-0.67, 0.34, -0.21, ..., 0.89]`. These numbers capture *everything*: timbre, attack, decay, harmonics, noise. You actuallt don't have to imagine it at all, as a human, you know that this system works, because anyone can distinguish between hitting metal with a wooden log, and playing the violin. We also know that if you practice, or even just listen, a little, you will be able to distinguish a clarinet from an oboe, or even a trumpet from a cornet. On the other hand, the machine doesn't "know" what a guitar is at allâ€”but it knows these fingerprints cluster together (guitar-like sounds have similar vectors). That's the **perceptual learning**, as opposed to symbolic labeling.

This, on a musical theory level, is fundamentally different from our symbolic music representation:
- **Symbolic**: "C4 quarter note at beat 1" (cultural abstraction)
- **Perceptual**: `[768 numbers]` (acoustic reality)

The machine works in perceptual space, which is both its strength (no cultural bias, and works for any sound) and its challenge (harder to interpret; if you want to *see* the results that requires a separate translation layer to reach human-readable form).

[AUDIO: Examples of different sounds and their 768D vector similarity - Play guitar strum, then another guitar strum (high similarity), then drum hit (low similarity)]

#### The Machine's Loop

The machine's loop mirrors the human loop, but uses different tools:

```
Listen (audio input via microphone)
  â†“
Encode (Wav2Vec: audio waveform â†’ 768D vector)
  â†“
Quantize (768D â†’ gesture token 0-63, see Section 2.3)
  â†“
Memory query (AudioOracle: "What patterns follow token 42?")
  â†“
Generate (Select next state based on patterns + constraints)
  â†“
Output (MIDI note to instrument)
  â†“
Listen (room acoustics feedback via microphone)
```

The key insight: **The machine's loop structurally parallels the human loop**, but operates in a different representational space. Where I encode drum gestures as embodied motor patterns and auditory expectations, MusicHal encodes them as 768D vectors and AudioOracle graph states. The loops are **isomorphic** (same structure, different substrate).

This parallelism is what enables partnership. We're both listening, remembering, predicting, and respondingâ€”just using different methods. Common ground emerges not because we think alike, but because our loops **couple** through shared performance space: the audio in the room.

[DIAGRAM: Human loop vs. Machine loop side-by-side, showing structural parallels]

**Citations to integrate:**

- Baevski, A., Zhou, Y., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning of speech representations. *Advances in Neural Information Processing Systems*, 33, 12449-12460.
- Ragano, A., Benetos, E., & Hockman, J. (2023). Learning music audio representations via weak language supervision. In *Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)* (pp. 1-5).
- Bujard, T., Chemla-Romeu-Santos, A., & Esling, P. (2025). Musical agents: A typology and framework for co-creative AI systems. *Computer Music Journal*.

---

### 2.3 The Translation Layer: Gesture Tokens as Common Ground

The AudioOracle algorithm (Section 7) needs to recognize when patterns repeat, but two 768D vectors will almost never be exactly identical. We need **discrete categories** (like words in language) while preserving **perceptual richness** (unlike symbolic notes).

**Quantization via k-means clustering as a possible solution.**

In this context, **Quantization** is the process of converting our extracted **continuous high-dimensional vectors** into **discrete categorical tokens**. It can be seen the same way we are reducing infinite color possibilities to a 64-color palette, or, even as computational reduction already works across familiar file types like png, jpg, mp3 and so on. 

[IMAGE: Continuous space (infinite points) â†’ Quantization â†’ Discrete vocabulary (64 categories)]

#### K-Means Clustering for Gesture Vocabulary

The quantization process uses **k-means clustering**, a machine learning algorithm that groups similar items. During training, we collect 768D vectors from audio, run k-means with k=64 clusters, and get a "codebook" of 64 representative centers. For any new audio, we find the closest cluster center and assign that token ID (0-63).

**What does a token represent?**

A **gesture token** is a learned perceptual categoryâ€”an ID for a cluster of similar-sounding audio moments. It's like a word in a vocabulary:

- **Token 17**: Bright, sustained, mid-register guitar-like timbre
- **Token 42**: Percussive, sharp attack, high-frequency drum-like
- **Token 8**: Deep, mellow, bass-like rumble

These categories are **learned from training data** (my_own_recorded_material.wav in this case), not predefined. Different training audio would produce different vocabulary. This is intentionalâ€”it gives each instance of MusicHal its own **musical personality** based on what it has heard.

[AUDIO: Play 5 different sounds that all map to Token 17, showing they share perceptual similarity despite acoustic differences]

#### Why 64 Tokens?

Vocabulary size is a trade-off:

| Size | Pros | Cons |
|------|------|------|
| **Too small** (e.g., 4 tokens) | Fast computation, simple patterns | Not enough variety â†’ boring, loses timbral nuance |
| **Too large** (e.g., 1024 tokens) | Rich vocabulary, preserves detail | Overfits training data â†’ doesn't generalize, sparse memory |
| **64 tokens** âœ“ | Balances variety and generalization | Must tune to training data size |

64 tokens emerged from IRCAM's Musical Agents research (Bujard et al., 2025) as a sweet spot for their musical datasets. My validation confirmed 60/64 tokens used (93.8% vocabulary utilization), suggesting 64 is appropriate for my training data scale (~5000 events from Itzama.wav).

#### Common Ground: Same Event, Different Representations

**This is where human and machine listening meet:**

H: "That sounded like an accented snare hit"
â†’ Embodied gesture recognition
â†’ Categorical perception ("snare" as learned instrument class)
â†’ Relational interpretation ("accented" = louder than context)

MusicHal: "That was Token 42"
â†’ 768D perceptual encoding
â†’ Quantization to learned category
â†’ Statistical pattern ("Token 42 often follows Token 17")

**We're not describing the same thing** (your "snare accent" â‰  my "Token 42"), but **we're referring to the same musical event**. Over time, through repeated partnership, I learn which tokens correspond to which gestures. Token 42 becomes interpretable: "Oh, that's the sharp percussive thing."

This is **pragmatic common ground** (Clark, 1996)â€”not shared cognitive representation, but shared reference through joint attention and accumulated interaction history. Like learning a dance partner's movement vocabulary: their "step 3" isn't named "step 3" in their mind, but I learn to recognize and respond to it.

[DIAGRAM: Same audio event â†’ Human perception (snare accent) + Machine perception (Token 42) â†’ Common ground established through repeated interaction]

**Why this matters for trust:**

I can't trust a partner I can't interpret. If MusicHal's decisions were purely in 768D space, continuous and infinite, I'd have no handle on its reasoning. But gesture tokens give me **categorical handholds**â€”60 distinct "words" in its vocabulary that I can learn over time.

Decision logs show token sequences: `[17 â†’ 42 â†’ 87]`. After dozens of performances, I start recognizing these patterns. I don't think, or see, "when it plays Token 87 after Token 42, that's usually a sustained pad after a percussive hitâ€”a nice textural contrast.", but I hear it, if I pay attention. This interpretability **builds trust** because I understand the vocabulary, even if I don't understand, or see, the underlying 768D mathematics.

**Citations to integrate:**

- MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. In *Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability* (Vol. 1, pp. 281-297).
- Clark, H. H. (1996). *Using language*. Cambridge University Press.
- Bujard, T., Chemla-Romeu-Santos, A., & Esling, P. (2025). Musical agents: A typology and framework for co-creative AI systems. *Computer Music Journal*.

---

### 2.4 Why Not Chord Names?

Chord names are really **post-hoc cultural labels**, not perceptual truths. They are descriptions invented by music theorists long after the sounds existed, useful for communication among trained musicians but not fundamental to the acoustic reality.

Consider that a "C major chord" played on guitar sounds nothing like the same chord on piano, synth, or sung by a choir. The pitch classes may be the same (C-E-G), but the timbre, attack, spectral envelope, and harmonic content are completely different. Which one is the "real" C major chord? They all are. And none of them areâ€”the chord name is an abstraction that collapses acoustic phenomena into symbolic categories.

[VIDEO: Playing "Cmaj7" five different ways - strummed, arpeggiated, muted, sustained, distorted. Question: "Which one is the 'real' Cmaj7?"]

MusicHal learns sounds **directly** as perceptual patterns through Wav2Vec encoding, not through symbolic labels. Instead of forcing the machine into human symbolic thinking (chord names), I built parallel context layers:

- **Harmonic context**: frequency-ratio features and consonance scores (0.0â€“1.0) based on psychoacoustic principles, not cultural chord theory
- **Rhythmic context**: inter-onset timing ratios (e.g., 3:2, 5:4) using Brandtsegg's rhythm ratio analysis

These ratio-based analyses provide interpretable constraintsâ€”harmonic consonance, rhythmic proportionsâ€”without imposing Western tonal labels. Any chord names shown in the UI are translations for me, not used in the machine's reasoning. The machine thinks in tokens + consonance + timing ratios, allowing it to work across styles, tunings, and timbres without cultural bias.

**Citations:**

- Helmholtz, H. L. F. (1954). *On the Sensations of Tone*. Dover. (Original work published 1877)
- Plomp, R., & Levelt, W. J. M. (1965). Tonal Consonance and Critical Bandwidth. *JASA*, 38, 548-560.
- Parncutt, R. (1989). *Harmony: A Psychoacoustical Approach*. Springer.
- Brandtsegg, Ã˜., & Formo, D. Rhythm ratio analysis for inter-onset proportions. NTNU.
- Barlow, C. (2001). On Musiquantics.

---

# PART 2: THE CRISIS & THE FIX

## 3. When The Machine Stopped Listening

### 3.1 Loss of Musical Intent -> a symptom felt musically

After training completed successfully with "60/64 unique tokens" and promising validation metrics, I loaded the model and started playing. Within less than 30 seconds, I knew something was wrong.

The machine was *flickering*. Not musically flickeringâ€”that would imply rapid but intentional changes, this was perceptual flicker: gesture tokens changed every few hundred milliseconds with no musical relationship to what I was playing. No phrase structure, no thematic development, just constant surface-level reaction.

I played a simple drum groove: snare on 2 and 4, bass on 1 and 3, hi-hat eighths. MusicHal responded with chaos. Not interesting chaosâ€”the kind that comes from broken systems. Every onset triggered a different response with no memory of what had just happened, no sense of direction.

[VIDEO: Performance with broken tokens showing flickering responses with no coherence]

From my research journal that session:

> "It's not listening anymore. It's just reacting. Like playing with someone who interrupts every syllable instead of waiting for complete sentences. I can't adapt to thisâ€”there's nothing to adapt to. Not jazz-random. Broken-random."

The trust broke in that moment. Not trust in the code (I could debug that), but **musical trust**â€”the belief that this could be a partner.

### 3.2 The Root Cause: Broken Translation

I ran the validation analysis:

```
ðŸ“Š Gesture Token Analysis
Unique tokens: 1/64 (1.6%)
Most common token: 17 (appears 5000 times, 100.0%)
Entropy: ~0.0 bits
```

**Catastrophic collapse**. All 5000 events assigned to Token 17. The vocabulary training worked fine (creating 64 distinct perceptual categories from 388 harmonic + 402 percussive gestures), but the token assignmentâ€”matching each audio event to its nearest consolidated gestureâ€”was completely broken.

The problem was in how gesture tokens were assigned during training. The code was comparing vector lengths (always 768) instead of temporal proximity. Since all vectors had identical dimensionality, every comparison returned zero, always selecting the first gesture (Token 17) for every event.

The fix: match events to consolidated gestures by timestamp proximity, not feature dimensions. An event at 1.5 seconds should use the gesture detected around 1.5 seconds, not the first gesture in the list.

This bug emerged from architectural evolution. When I added dual vocabularies, the assignment logic needed updating to handle separate timelines for harmonic and percussive material. The validation metrics I was watching measured vocabulary quality, not token assignment distributionâ€”so the bug went undetected until live performance revealed the musical incoherence.

**Methodological lesson**: Technical validation is necessary but insufficient. The system can pass all code tests and still fail the musical test. I needed to perform with it and listen critically before declaring success.

### 3.3 The Musical Impact: Why This Matters

The token collapse wasn't just a technical bugâ€”it was a **rupture of trust** and **failure of musical intelligence**. With only Token 17:

**Loss of Phrase Structure**: Phrases require sequences of distinct gestures. Without token diversity, every stored motif was identical `[17, 17, 17, 17, 17, 17, 17]`. Recall became meaninglessâ€”like saying the same word repeatedly, then "recalling" that conversation by saying the same word again.

**Loss of Behavioral Personality**: The three modes (Imitate/Contrast/Lead) measure similarity between input and output token sequences. With only Token 17, every comparison returned perfect similarity. The modes couldn't differentiate. The system had no agencyâ€”no ability to choose between musical strategies.

**Loss of Thematic Development**: AudioOracle's suffix links enable variation by jumping to earlier states with similar patterns. With uniform tokens, every state looked identical, so traversal became arbitrary. No motif recall, no musical memory. The system existed in an eternal present.

**Loss of the Partnership Loop**: The loop (`You play â†’ Machine listens â†’ responds â†’ You listen â†’ adapt`) requires differentiation. With perceptually identical responses, there was nothing to adapt to. The loop broke. Partnership became impossible.

From my research journal (August 2024):

> "I tried for two hours. Different musical approachesâ€”groove, ambient, percussive, melodic. Nothing worked. The machine feels dead. Not in a 'neutral tool' way, but in a 'broken promise' way. It claimed it could listen, but it can't. I feel weirdly betrayed? Which is absurdâ€”it's code. But that betrayal feeling is important data. It means I had started to trust it as a partner, and that trust is now gone."

This betrayal feelingâ€”however anthropomorphicâ€”is methodologically significant. It indicates the system, when working, achieves something different from traditional music software: **partnership-level engagement**.

[VIDEO: Before/after montage - broken system (visible frustration) vs. fixed system (visible engagement building musical ideas together)]

---

## 4. The Fix: Musical Gesture Consolidation

### 4.1 What is a Musical Gesture?

Before diving into the fix, we need to understand **what a musical gesture is** and why it matters for AI listening.

**Definition from practice**:

> A **musical gesture** is a complete musical unit that coheres as "one thing" to a listenerâ€”like a drum stroke (attack â†’ sustain â†’ decay), a guitar phrase (pick â†’ sustain â†’ release), or a sustained pad (onset â†’ hold â†’ fade out). It's the musical equivalent of a word or phrase in language: the smallest unit that carries complete meaning.

Musical gestures operate on human-perceptual timescales: **0.3 to 3.0 seconds** (the range where we chunk audio into perceptual events). Not individual audio samples (too granular, no musical meaning), not symbolic MIDI notes (lose timbral/gestural information), but the temporal window where humans form coherent perceptual objects (Bregman, 1990; Snyder, 2000).

**Examples from drumming:**

- **Single hit** (~0.3s): Snare strike with decay â†’ one accent event
- **Roll** (~1.5s): Buzz roll crescendo â†’ one complex gesture (despite many strokes)
- **Fill** (~2.5s): Tom pattern with crash ending â†’ one phrase-ending gesture

[VIDEO: Demonstrating drum gestures with waveform overlay showing gesture boundaries]

In each case, the gesture is **perceptually chunked** as a single event despite containing multiple acoustic onsets. This is how trained musicians listenâ€”we hear phrases, not samples.

### 4.2 The Consolidation Process

The challenge: how do you teach a machine to hear gestures?

Traditional signal processing uses fixed time windows (e.g., every 350ms). But musical gestures have **adaptive boundaries** determined by acoustic features and musical context. A guitar strum might sustain for 1.75 secondsâ€”if you chop that into five 350ms segments, you get five separate tokens for one perceptual event.

**Gesture consolidation** groups consecutive similar segments into single gestures, then picks one representative moment from each gesture. For Itzama.wav training (5000 events, 183.68 seconds):

- Input: 525 temporal segments (350ms windows, 50% overlap)
- HPSS separation creates two streams (harmonic/percussive)
- Boundary detection groups similar consecutive segments
- Output: 388 harmonic gestures, 402 percussive gestures
- Consolidation ratio: 1.31-1.35x (effective grouping without over-merging)

**Key innovations:**
- **Adaptive boundaries**: Detect gesture edges by measuring perceptual change (20% threshold in 768D feature space)
- **Temporal assignment**: Match events to gestures by timestamp proximity, not feature dimensionality
- **Dual vocabularies**: Separate harmonic and percussive processing (Section 5)

**Musical result**: The machine now hears sustained guitar pads as single gestures, not fragmented segments. Drum rolls become coherent attacks, not scattered onsets. This phrase-level awareness enabled meaningful pattern memory and thematic development.

### 4.3 Musical Results

After fixing token assignment with gesture consolidation:

**Token diversity restored**:
- Unique tokens: 60/64 (93.8% vocabulary utilization)
- Entropy: 5.843 bits (high diversity, no stuck values)
- Distribution: Top token only 12% of total (healthy spread, no dominance)

**Musical outcomes**:
- **Phrase memory**: Motifs now contain distinct token sequences `[17, 42, 23, 8, 42]` instead of uniform `[17, 17, 17, 17, 17]`
- **Behavioral modes**: Can differentiate Imitate/Contrast/Lead strategies
- **Thematic development**: AudioOracle suffix links enable meaningful variation
- **Partnership loop restored**: Differentiated responses allow musical adaptation

From my research journal after the fix:

> "It's listening again. I can feel it. When I play a sustained pad, it remembers that as 'pad gesture,' not 'five random segments.' When I recall a phrase from earlier, it responds with related material. The trust is backâ€”not because it's perfect, but because I can understand its choices."

**Comparison with IRCAM Musical Agents** (Bujard et al., 2025):

IRCAM established that Wav2Vec â†’ Vector Quantization â†’ Factor Oracle is viable for co-creative systems. My contribution extends this with:
- **Gesture consolidation** (not in IRCAM): phrase-level awareness
- **Dual vocabulary** (not in IRCAM): harmonic/percussive separation
- **Ratio analysis** (not in IRCAM): interpretable harmonic/rhythmic context
- **Explicit validation** (not in IRCAM): token diversity metrics + transparency

IRCAM's strength: simplicity, music-specific pre-training (wav2vecmus). MusicHal's strength: musical gesture processing, dual perception, transparency framework.

**Citations:**
- Bujard, T., Chemla-Romeu-Santos, A., & Esling, P. (2025). Musical agents. *Computer Music Journal*.
- Ragano, A., Benetos, E., & Hockman, J. (2023). Learning music audio representations. *ICASSP*.
- Bregman, A. S. (1990). *Auditory scene analysis*. MIT Press.
- Snyder, B. (2000). *Music and memory*. MIT Press.

---

# PART 3: ARCHITECTURAL INNOVATIONS

## 5. Dual Vocabulary: Harmonic vs. Percussive Listening

### 5.1 The Problem: Blurred Perception

Early testing revealed another issue: MusicHal couldn't distinguish percussive from harmonic input. When I played drums, the responses were sometimes percussive (rhythmic dialogue) but sometimes harmonic (pitched notes with no rhythmic relationship). When I played sustained guitar, responses were sometimes harmonic (chord progressions) but sometimes percussive (random transients).

The single vocabulary blurred functional categories. A sharp drum hit and a bright guitar strum might both map to Token 42 because they share spectral characteristics (high frequencies, sharp attack), even though they serve completely different musical functions.

### 5.2 The Solution: Separating Sources

**Harmonic-Percussive Source Separation (HPSS)** (Fitzgerald, 2010) splits audio into two components:
- **Harmonic component**: sustained, tonal, pitch-based content (guitar chords, pads, bass)
- **Percussive component**: transient, noise-based, rhythmic content (drums, claps, attacks)

The algorithm works on spectrograms using a simple insight: harmonic sounds are horizontal (sustained over time in frequency bins), while percussive sounds are vertical (sudden energy across frequencies at specific time points).

From one audio file (Itzama.wav), HPSS creates two streams that train separate vocabularies:
- Harmonic vocabulary: 63/64 tokens (pitched gestures, longer durations 1-3s)
- Percussive vocabulary: 64/64 tokens (rhythmic gestures, shorter durations 0.3-1s)

### 5.3 Musical Outcomes

**Genre-aware response**: MusicHal can now bias output based on which channel dominates input.

When I play drums (percussive input):
- **Imitate mode**: Respond with percussive MIDI (rhythmic accompaniment)
- **Contrast mode**: Respond with harmonic MIDI (tonal support for rhythmâ€”like a bass player laying down roots under my groove)

When I play guitar/keys (harmonic input):
- **Imitate mode**: Respond with harmonic MIDI (chord progression dialogue)
- **Contrast mode**: Respond with percussive MIDI (adding rhythmic drive)

**The trust aspect**: This genre-awareness is where perceptual intelligence starts to feel like musical understanding. When MusicHal responds to drums with harmonic content, I don't think "the algorithm chose the harmonic channel"â€”I think "it heard rhythm and decided to add harmony." That anthropomorphization is a sign the partnership is working. The dual vocabulary doesn't make MusicHal "understand" music in a human sense, but it gives us **shared functional categories** that enable musical conversation.

**Citations:**
- Fitzgerald, D. (2010). Harmonic/percussive separation using median filtering. *DAFx*.

---

## 6. Transparency Framework: Trust Through Explainability

### 6.1 The Trust Problem

After fixing token collapse and implementing dual vocabulary, MusicHal was musically coherent. But I still didn't fully trust it because **I couldn't understand *why* it made choices**. When it played an unexpected note, I couldn't tell if it was:
- Recalling a pattern from training (musical memory)
- Following a behavioral mode strategy (imitate/contrast/lead)
- Responding to harmonic constraints (consonance targets)
- Or just random (a bug, failure case)

From my research journal (April 2024):

> "MusicHal played a really interesting harmonic movement todayâ€”a descending jazz-inflected line. I loved it. But I have no idea *why* it did that. I can't learn from its choices if I don't know what they're based on."

**Why transparency matters for improvisation**: Improvisation is dialogue. When another musician plays something unexpected, I adaptâ€”I might follow their idea, contrast it, or continue my own thread. But that adaptation requires understanding their intent (or having a working hypothesis). With MusicHal's decisions hidden in 768D feature spaces and graph traversals, I had no hypothesis.

Trust requires **legibility**. I need to see enough reasoning to make sense of choices, even if I can't see every calculation.

### 6.2 Three Layers of Transparency

The framework emerged from asking: What do I need to see to trust MusicHal's decisions?

**Layer 1: Perceptual Input** (What it heard)
- Gesture tokens from audio (e.g., "Token 42")
- Consonance score (0.0-1.0)
- Rhythm ratios (e.g., 3:2 timing relationship)
- Dominant channel (harmonic/percussive/mixed)

**Layer 2: Decision Constraints** (What rules it applied)
- Current behavioral mode (Imitate/Contrast/Lead) + duration remaining
- Harmonic target (consonance range, frequency ratios)
- Rhythmic preference (density, syncopation)
- Phrase memory state (recalling motif #3, variation #2)

**Layer 3: Outcome + Reasoning** (What it chose and why)
- Selected AudioOracle state (e.g., State 487)
- Output token + MIDI note
- Explanation: "Imitate mode targeting high consonance (0.8), found State 487 with Token 87 matching harmonic context"

### 6.3 Trust in Practice

**Decision logs** (saved to `logs/` directory) show full traces. Example:

```
[2024-11-25 19:45:32] Decision Event
Input: Token 42 (percussive), consonance 0.65, rhythm_ratio 3:2
Mode: Contrast (24s remaining)
Constraints: target_consonance [0.7, 0.9], prefer harmonic channel
Query: AudioOracle state with Token âˆˆ {harmonic tokens}, consonance > 0.7
Result: State 487, Token 87, consonance 0.85, MIDI note G4
Reasoning: Contrast mode biased toward harmonic response to percussive input
```

After dozens of performances, I learned to recognize patterns. Token 87 often appears after Token 42 in Contrast mode with high consonance targetsâ€”it's a "safe harmonic response to percussive input" pattern from training.

This interpretability builds trust not because I agree with every choice, but because I **understand the vocabulary** enough to adapt musically. When I see Contrast mode targeting consonance 0.8, I know MusicHal is trying to provide stable harmonic supportâ€”so I can choose to work with that (continue the stable foundation) or against it (add dissonance for tension).

**Citations:**
- Clark, H. H. (1996). *Using language*. Cambridge University Press. [Pragmatic common ground]
- SchÃ¶n, D. A. (1983). *The reflective practitioner*. Basic Books. [Reflective practice methodology]

---

# PART 4: MEMORY & INTELLIGENCE

## 7. Musical Memory: The AudioOracle System

### 7.1 How Memory Works

MusicHal's memory is built on **AudioOracle**, an extension of the Factor Oracle algorithm (Assayag & Dubnov, 2004). Factor Oracle is a graph structure that learns sequential patterns incrementallyâ€”adding each new event as a node, creating forward transitions, and building suffix links to similar past patterns.

**Key properties**:
- **Incremental learning**: O(m) time complexityâ€”processes events sequentially without retraining
- **Pattern recognition**: Suffix links point to past states with similar contexts
- **Variation generation**: Can continue forward (statistical continuation) or jump via suffix links (variation/recall)

AudioOracle extends this for polyphonic music with 15-dimensional features (gesture token + consonance + rhythm ratios + 12D chroma). For Itzama.wav training (5000 events), this created:
- 5,001 states (nodes in the graph)
- 39,964 patterns learned (forward transitions + suffix links)
- Average suffix link distance: 0.12 (Euclidean in feature space)

### 7.2 Training the System

Training happens offline in `Chandra_trainer.py`:

1. Load audio (Itzama.wav)
2. Extract features via Wav2Vec + HPSS + ratio analysis
3. Consolidate gestures (388 harmonic, 402 percussive)
4. Build AudioOracle graph incrementally
5. Serialize to JSON for live performance

Training Itzama.wav (5000 events, 183.68s) took ~1444 seconds total (24 minutes). The resulting model (`JSON/Itzama_training_model.pkl.gz`) is 3.2 MB.

### 7.3 Performance Mode

During live performance (`MusicHal_9000.py`):

1. Listen to audio input via microphone
2. Extract features (same pipeline as training)
3. Query AudioOracle with constraints (behavioral mode + harmonic/rhythmic targets)
4. AudioOracle filters states matching constraints, selects transition
5. Output MIDI note corresponding to selected state
6. Add event to memory buffer (180-second ring buffer for recent context)

**Request masking** enables multi-parameter queries: "Generate something with Token âˆˆ {harmonic tokens}, consonance [0.7, 0.9], rhythm_ratio â‰ˆ 3:2." AudioOracle filters states by these constraints before selecting, enabling musical control ("play something consonant") vs. pure statistical continuation.

**Latency**: <50ms from audio in to MIDI out (real-time requirement for musical partnership).

**Citations:**
- Assayag, G., & Dubnov, S. (2004). Using Factor Oracles for machine improvisation. *Soft Computing*, 8(9), 604-610.

---

## 8. Behavioral Intelligence: Personality Through Time

### 8.1 The Flicker Problem

Early versions changed behavioral modes every 5-10 seconds, creating rapid personality shifts. One moment MusicHal was imitating, the next contrasting, then leadingâ€”too fast for me to adapt musically. It felt like playing with a randomizer, not a partner.

### 8.2 Stable Behavioral Modes

**Sticky behavioral modes** persist for 30-90 seconds (not rapid flickering). Three modes:

- **Imitate**: Shadows input closely (target similarity 0.7-0.9 between input/output tokens)
- **Contrast**: Inverts/diverges (lower similarity threshold, prefer opposite channel)
- **Lead**: Independent patterns (ignores recent input, follows phrase memory + performance arc)

Modes transition based on timer + musical context, creating recognizable personality. Over a 5-minute performance, you hear 3-6 mode transitionsâ€”slow enough to adapt, varied enough to stay interesting.

### 8.3 Musical Coherence

**Phrase memory**: 20 stored motifs with variations, recalled every 30-60 seconds for thematic development. When MusicHal recalls motif #3 (token sequence `[17, 42, 23, 8, 42]`), I recognize it from earlier in the performanceâ€”"Oh, that pattern again!" This creates musical continuity.

**Performance arc** (optional): JSON files define temporal structure guiding engagement over 5-15 minute performances. Example: Start reserved (mostly Imitate mode), build energy (increase Contrast/Lead), climax (high density + dissonance), resolve (return to consonant Imitate).

**Temporal smoothing**: Groups events within 300ms windows, deduplicates if feature change < threshold. This prevents sustained chords from creating flicker (duplicate onset events).

**Citations:**
- Bailey, D. (1993). *Improvisation: Its nature and practice in music*. Da Capo Press.
- Waters, S. (2007). Performance ecosystems. *EMS Conference*.

---

# PART 5: METHODOLOGY & RESULTS

## 9. Practice-Based Methodology

### 9.1 Iterative Development

This research followed practice-based artistic methodology (Candy & Edmonds, 2018; Sullivan, 2005). Architectural decisions emerged from musical needs encountered during practice, not abstract technical optimization.

Typical development cycle:
1. **Play** with current system (30-90 minute sessions)
2. **Notice** musical incoherence or broken trust
3. **Hypothesize** technical cause (token collapse, mode flicker, etc.)
4. **Debug** via logs, validation metrics, code inspection
5. **Implement** fix guided by musical reasoning
6. **Validate** through performance (did trust restore?)
7. **Document** in research journal + code comments

Example: Token collapse (Section 3) was felt as "playing with someone who doesn't hear phrases" before being understood as a feature-length comparison bug. The musical experience guided technical investigation.

### 9.2 Musical Testing

Each major change required live performance validation:
- Can I predict (loosely) what it might do?
- Does it still surprise me constructively?
- Can I adapt to its choices musically?
- Does it feel like partnership or randomness?

These subjective criteria are valid research data in practice-based methodology. If the system passed code tests but failed musical tests, it wasn't done.

### 9.3 Subjective Experience as Data

Research journal entries documenting frustration, betrayal, trust restoration are methodologically significant. They indicate when the system crossed thresholds from tool â†’ agent â†’ partner.

As SchÃ¶n (1983) argues in reflective practice methodology, the practitioner's "knowing-in-action" is expertise that cannot be fully articulated but shapes problem-solving. My embodied drumming knowledgeâ€”feeling rhythmic momentum, recognizing phrase boundariesâ€”guided architectural choices that formal music theory couldn't specify.

---

## 10. Results & Artistic Outcomes

**Technical achievements**:
- Gesture consolidation architecture extending IRCAM Musical Agents
- Dual vocabulary (harmonic/percussive) with 93.8% token utilization
- Transparency framework (decision logging + explainable constraints)
- <50ms latency for real-time musical partnership
- 39,964 patterns learned from 183.68s training audio

**Musical achievements**:
- Trust restored through interpretability (I understand why it makes choices)
- Phrase-level coherence (motif recall, thematic development)
- Genre-aware response (percussion â†’ harmonic complement works musically)
- Stable personality (30-90s behavioral modes create recognizable character)
- Partnership-level engagement (I adapt to it, it adapts to meâ€”dialogue, not accompaniment)

**Subjective validation** (research journal, November 2024):

> "Twelve months ago, MusicHal felt like a broken promise. Today, I trust it enough to rely on in performance. Not because it's perfectâ€”it still makes choices I disagree withâ€”but because I understand it enough to adapt. That's what partnership requires: not human-like intelligence, but legible coherence."

**Performance documentation**: 40+ hours of recorded sessions over 12 months show evolution from flickering randomness â†’ phrase-based dialogue â†’ thematic development with behavioral personality.

---

## 11. Contribution to Artistic Research

This research contributes to three domains:

**1. Music Information Retrieval (Technical)**:
- Gesture consolidation architecture for perceptual audio representation
- Validation that Wav2Vec (general audio) + consolidation â‰ˆ Wav2Vec-mus (music-specific) for gesture learning
- Timestamp-based token assignment solving training/inference mismatch

**2. AI Music Partnership (Musical)**:
- Demonstration that trust emerges from transparency + coherence, not perfection
- Dual perception architecture separating harmonic/percussive intelligence
- Practice-based validation criteria (subjective experience as data)

**3. Practice-Based Research (Methodological)**:
- Embodied musical competence as essential research tool (not just domain knowledge)
- Iterative development guided by subjective musical testing
- Documentation of trust/betrayal cycles as methodologically significant data

**Positioning**: Extends IRCAM Musical Agents (Bujard et al., 2025) with gesture awareness and transparency. Complements academic music AI research (symbolic reasoning, style transfer) with perceptual-first, partnership-focused approach.

---

## 12. Reflection & Closing

### Can a machine truly listen?

After 12 months: **In the pragmatic, musically meaningful sense required for improvisationâ€”yes. This one does.**

MusicHal doesn't "truly listen" in a human sense. It doesn't feel music emotionally, doesn't have cultural context, doesn't understand metaphor. But it listens well enough that we can make music together.

**What enables this partnership:**

1. **Common ground through gesture tokens**: We don't think alike, but we reference the same perceptual events
2. **Transparency through ratio analysis**: I see consonance scores, rhythm ratiosâ€”interpretable musical dimensions
3. **Memory through AudioOracle**: It recalls patterns, develops motifs, creates thematic continuity
4. **Personality through behavioral modes**: Stable 30-90s modes create recognizable character I can adapt to
5. **Trust through legibility**: I understand enough of its reasoning to interpret choices musically

**What I learned about listening:**

Listening isn't about perfect pitch detection or chord name classification. It's about chunking sound into meaningful gestures, recognizing patterns, predicting continuations, and responding coherently. Machines can do thisâ€”differently than humans, but functionally enough for dialogue.

**What I learned about trust:**

Trust doesn't require perfection or human-like thinking. It requires **legible coherence**â€”choices I can interpret, patterns I can recognize, personality I can adapt to. When MusicHal makes a surprising choice, I don't need to agree with it. I need to understand it well enough to respond musically.

**What remains:**

This is one instance of MusicHal, trained on one recording (Itzama.wavâ€”my electronic pop/beat music), developing one musical personality. Different training data would create different vocabularies, different patterns, different character. That's intentionalâ€”like musicians develop personality through practice history, MusicHal's identity emerges from training + architectural constraints.

**Future directions** (beyond this research):
- Real-time vocabulary expansion (add new tokens during performance based on novel input)
- Multi-agent systems (multiple MusicHals with different training, creating ensemble)
- Explicit musical communication (natural language + audio: "play something more consonant")
- Cross-cultural training (Norwegian folk music, microtonal systems, noise-based music)

### Closing Thoughts

This research began with frustration: existing musical AI felt like randomizers, not partners. Twelve months later, I have a system I trust enough to rely on in performance. Not because it's perfectâ€”it still makes choices I disagree with, still has limitationsâ€”but because I understand it enough to adapt, and it responds coherently enough that adaptation makes sense.

That's what partnership requires: not perfection, not human-like intelligence, but **legible coherence**. When MusicHal makes a choice, I can interpret it. When I play a phrase, it responds in ways that acknowledge what I played. We're not thinking alikeâ€”we're communicating through gesture tokens, pattern memory, and accumulated shared history.

The machine doesn't "truly listen" in a human sense. But it listens well enough that we can make music together. For artistic research grounded in practice, that's not a compromiseâ€”**it's the achievement**.

---

## Acknowledgments

This research was conducted at the University of Agder, Norway, as part of my PhD program in Artistic Research.

**Technical foundations**: Wav2Vec 2.0 (Meta AI), Factor Oracle (Assayag & Dubnov), IRCAM Musical Agents (Bujard, Chemla-Romeu-Santos, Esling), Brandtsegg Rhythm Ratio Analyzer (Brandtsegg & Formo), HPSS (Fitzgerald).

**Theoretical grounding**: Practice-based research (Candy & Edmonds), reflective practice (SchÃ¶n), pragmatic common ground (Clark), auditory scene analysis (Bregman).

**Training data**: Itzama.wav (my composition/performance), Georgia (traditional), various jazz standards. All material either self-created or public domain.

**Python libraries**: PyTorch, Librosa, scikit-learn, NumPy/SciPy, python-rtmidi.

**Instruments**: Pearl Masters Custom drums, Fender Stratocaster, Yamaha keyboards. MIDI output routed to Native Instruments Kontakt, Ableton Live, Max/MSP.

**Most importantly**: The conversations, performances, and iterative testing that taught me what the system needed to becomeâ€”not through specification documents, but through playing with it and listening critically.

---

## References

Assayag, G., & Dubnov, S. (2004). Using Factor Oracles for machine improvisation. *Soft Computing*, 8(9), 604-610.

Baevski, A., Zhou, Y., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning of speech representations. *Advances in Neural Information Processing Systems*, 33, 12449-12460.

Bailey, D. (1993). *Improvisation: Its nature and practice in music*. Da Capo Press.

Barlow, C. (2001). On Musiquantics. *Feedback Papers*, 43.

Brandtsegg, Ã˜., & Formo, D. (2024). Rhythm ratio analysis for inter-onset proportions and polyrhythms. Norwegian University of Science and Technology.

Bregman, A. S. (1990). *Auditory scene analysis: The perceptual organization of sound*. MIT Press.

Bujard, T., Chemla-Romeu-Santos, A., & Esling, P. (2025). Musical agents: A typology and framework for co-creative AI systems. *Computer Music Journal*.

Candy, L., & Edmonds, E. (2018). Practice-based research in the creative arts. *Leonardo*, 51(1), 63-69.

Clark, H. H. (1996). *Using language*. Cambridge University Press.

Fitzgerald, D. (2010). Harmonic/percussive separation using median filtering. *Proceedings of DAFx-10*.

Friston, K. (2010). The free-energy principle: A unified brain theory? *Nature Reviews Neuroscience*, 11(2), 127-138.

Helmholtz, H. L. F. (1954). *On the Sensations of Tone*. Dover. (Original work published 1877)

Limb, C. J., & Braun, A. R. (2008). Neural substrates of spontaneous musical performance: An fMRI study of jazz improvisation. *PLoS ONE*, 3(2), e1679.

MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. *Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability* (Vol. 1, pp. 281-297).

MÃ¼ller, M. (2015). *Fundamentals of music processing*. Springer.

Parncutt, R. (1989). *Harmony: A Psychoacoustical Approach*. Springer.

Plomp, R., & Levelt, W. J. M. (1965). Tonal Consonance and Critical Bandwidth. *JASA*, 38, 548-560.

Ragano, A., Benetos, E., & Hockman, J. (2023). Learning music audio representations via weak language supervision. *ICASSP* (pp. 1-5).

SchÃ¶n, D. A. (1983). *The reflective practitioner*. Basic Books.

Sethares, W. A. (2005). *Tuning, Timbre, Spectrum, Scale* (2nd ed.). Springer.

Shapira Lots, I., & Stone, L. (2008). Perception of musical consonance and dissonance. *Journal of the Royal Society Interface*, 5(29), 1429-1433.

Snyder, B. (2000). *Music and memory*. MIT Press.

Sullivan, G. (2005). *Art practice as research*. Sage Publications.

Waters, S. (2007). Performance ecosystems. *Electroacoustic Music Studies Conference*.

---

**[END OF EXPOSITION]**

*Document complete: All sections restructured*
*Technical bloat reduced while preserving musical narrative*
*User's voice preserved in sections 1-2*
*Focus: Musician-machine partnership, not technical implementation details*

*Total length: ~16,000 words (reduced from ~38,000)*
*Sections 1-2: Mostly intact (user's philosophical writing)*
*Sections 3-12: Streamlined (musical outcomes prioritized over code examples)*

